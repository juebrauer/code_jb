{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15a2cc8",
   "metadata": {},
   "source": [
    "# Schichten eines YOLO11 Modells anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a99127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 model.model.0.conv.weight torch.Size([16, 3, 3, 3])\n",
      "1 model.model.0.bn.weight torch.Size([16])\n",
      "2 model.model.0.bn.bias torch.Size([16])\n",
      "3 model.model.1.conv.weight torch.Size([32, 16, 3, 3])\n",
      "4 model.model.1.bn.weight torch.Size([32])\n",
      "5 model.model.1.bn.bias torch.Size([32])\n",
      "6 model.model.2.cv1.conv.weight torch.Size([32, 32, 1, 1])\n",
      "7 model.model.2.cv1.bn.weight torch.Size([32])\n",
      "8 model.model.2.cv1.bn.bias torch.Size([32])\n",
      "9 model.model.2.cv2.conv.weight torch.Size([64, 48, 1, 1])\n",
      "10 model.model.2.cv2.bn.weight torch.Size([64])\n",
      "11 model.model.2.cv2.bn.bias torch.Size([64])\n",
      "12 model.model.2.m.0.cv1.conv.weight torch.Size([8, 16, 3, 3])\n",
      "13 model.model.2.m.0.cv1.bn.weight torch.Size([8])\n",
      "14 model.model.2.m.0.cv1.bn.bias torch.Size([8])\n",
      "15 model.model.2.m.0.cv2.conv.weight torch.Size([16, 8, 3, 3])\n",
      "16 model.model.2.m.0.cv2.bn.weight torch.Size([16])\n",
      "17 model.model.2.m.0.cv2.bn.bias torch.Size([16])\n",
      "18 model.model.3.conv.weight torch.Size([64, 64, 3, 3])\n",
      "19 model.model.3.bn.weight torch.Size([64])\n",
      "20 model.model.3.bn.bias torch.Size([64])\n",
      "21 model.model.4.cv1.conv.weight torch.Size([64, 64, 1, 1])\n",
      "22 model.model.4.cv1.bn.weight torch.Size([64])\n",
      "23 model.model.4.cv1.bn.bias torch.Size([64])\n",
      "24 model.model.4.cv2.conv.weight torch.Size([128, 96, 1, 1])\n",
      "25 model.model.4.cv2.bn.weight torch.Size([128])\n",
      "26 model.model.4.cv2.bn.bias torch.Size([128])\n",
      "27 model.model.4.m.0.cv1.conv.weight torch.Size([16, 32, 3, 3])\n",
      "28 model.model.4.m.0.cv1.bn.weight torch.Size([16])\n",
      "29 model.model.4.m.0.cv1.bn.bias torch.Size([16])\n",
      "30 model.model.4.m.0.cv2.conv.weight torch.Size([32, 16, 3, 3])\n",
      "31 model.model.4.m.0.cv2.bn.weight torch.Size([32])\n",
      "32 model.model.4.m.0.cv2.bn.bias torch.Size([32])\n",
      "33 model.model.5.conv.weight torch.Size([128, 128, 3, 3])\n",
      "34 model.model.5.bn.weight torch.Size([128])\n",
      "35 model.model.5.bn.bias torch.Size([128])\n",
      "36 model.model.6.cv1.conv.weight torch.Size([128, 128, 1, 1])\n",
      "37 model.model.6.cv1.bn.weight torch.Size([128])\n",
      "38 model.model.6.cv1.bn.bias torch.Size([128])\n",
      "39 model.model.6.cv2.conv.weight torch.Size([128, 192, 1, 1])\n",
      "40 model.model.6.cv2.bn.weight torch.Size([128])\n",
      "41 model.model.6.cv2.bn.bias torch.Size([128])\n",
      "42 model.model.6.m.0.cv1.conv.weight torch.Size([32, 64, 1, 1])\n",
      "43 model.model.6.m.0.cv1.bn.weight torch.Size([32])\n",
      "44 model.model.6.m.0.cv1.bn.bias torch.Size([32])\n",
      "45 model.model.6.m.0.cv2.conv.weight torch.Size([32, 64, 1, 1])\n",
      "46 model.model.6.m.0.cv2.bn.weight torch.Size([32])\n",
      "47 model.model.6.m.0.cv2.bn.bias torch.Size([32])\n",
      "48 model.model.6.m.0.cv3.conv.weight torch.Size([64, 64, 1, 1])\n",
      "49 model.model.6.m.0.cv3.bn.weight torch.Size([64])\n",
      "50 model.model.6.m.0.cv3.bn.bias torch.Size([64])\n",
      "51 model.model.6.m.0.m.0.cv1.conv.weight torch.Size([32, 32, 3, 3])\n",
      "52 model.model.6.m.0.m.0.cv1.bn.weight torch.Size([32])\n",
      "53 model.model.6.m.0.m.0.cv1.bn.bias torch.Size([32])\n",
      "54 model.model.6.m.0.m.0.cv2.conv.weight torch.Size([32, 32, 3, 3])\n",
      "55 model.model.6.m.0.m.0.cv2.bn.weight torch.Size([32])\n",
      "56 model.model.6.m.0.m.0.cv2.bn.bias torch.Size([32])\n",
      "57 model.model.6.m.0.m.1.cv1.conv.weight torch.Size([32, 32, 3, 3])\n",
      "58 model.model.6.m.0.m.1.cv1.bn.weight torch.Size([32])\n",
      "59 model.model.6.m.0.m.1.cv1.bn.bias torch.Size([32])\n",
      "60 model.model.6.m.0.m.1.cv2.conv.weight torch.Size([32, 32, 3, 3])\n",
      "61 model.model.6.m.0.m.1.cv2.bn.weight torch.Size([32])\n",
      "62 model.model.6.m.0.m.1.cv2.bn.bias torch.Size([32])\n",
      "63 model.model.7.conv.weight torch.Size([256, 128, 3, 3])\n",
      "64 model.model.7.bn.weight torch.Size([256])\n",
      "65 model.model.7.bn.bias torch.Size([256])\n",
      "66 model.model.8.cv1.conv.weight torch.Size([256, 256, 1, 1])\n",
      "67 model.model.8.cv1.bn.weight torch.Size([256])\n",
      "68 model.model.8.cv1.bn.bias torch.Size([256])\n",
      "69 model.model.8.cv2.conv.weight torch.Size([256, 384, 1, 1])\n",
      "70 model.model.8.cv2.bn.weight torch.Size([256])\n",
      "71 model.model.8.cv2.bn.bias torch.Size([256])\n",
      "72 model.model.8.m.0.cv1.conv.weight torch.Size([64, 128, 1, 1])\n",
      "73 model.model.8.m.0.cv1.bn.weight torch.Size([64])\n",
      "74 model.model.8.m.0.cv1.bn.bias torch.Size([64])\n",
      "75 model.model.8.m.0.cv2.conv.weight torch.Size([64, 128, 1, 1])\n",
      "76 model.model.8.m.0.cv2.bn.weight torch.Size([64])\n",
      "77 model.model.8.m.0.cv2.bn.bias torch.Size([64])\n",
      "78 model.model.8.m.0.cv3.conv.weight torch.Size([128, 128, 1, 1])\n",
      "79 model.model.8.m.0.cv3.bn.weight torch.Size([128])\n",
      "80 model.model.8.m.0.cv3.bn.bias torch.Size([128])\n",
      "81 model.model.8.m.0.m.0.cv1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "82 model.model.8.m.0.m.0.cv1.bn.weight torch.Size([64])\n",
      "83 model.model.8.m.0.m.0.cv1.bn.bias torch.Size([64])\n",
      "84 model.model.8.m.0.m.0.cv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "85 model.model.8.m.0.m.0.cv2.bn.weight torch.Size([64])\n",
      "86 model.model.8.m.0.m.0.cv2.bn.bias torch.Size([64])\n",
      "87 model.model.8.m.0.m.1.cv1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "88 model.model.8.m.0.m.1.cv1.bn.weight torch.Size([64])\n",
      "89 model.model.8.m.0.m.1.cv1.bn.bias torch.Size([64])\n",
      "90 model.model.8.m.0.m.1.cv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "91 model.model.8.m.0.m.1.cv2.bn.weight torch.Size([64])\n",
      "92 model.model.8.m.0.m.1.cv2.bn.bias torch.Size([64])\n",
      "93 model.model.9.cv1.conv.weight torch.Size([128, 256, 1, 1])\n",
      "94 model.model.9.cv1.bn.weight torch.Size([128])\n",
      "95 model.model.9.cv1.bn.bias torch.Size([128])\n",
      "96 model.model.9.cv2.conv.weight torch.Size([256, 512, 1, 1])\n",
      "97 model.model.9.cv2.bn.weight torch.Size([256])\n",
      "98 model.model.9.cv2.bn.bias torch.Size([256])\n",
      "99 model.model.10.cv1.conv.weight torch.Size([256, 256, 1, 1])\n",
      "100 model.model.10.cv1.bn.weight torch.Size([256])\n",
      "101 model.model.10.cv1.bn.bias torch.Size([256])\n",
      "102 model.model.10.cv2.conv.weight torch.Size([256, 256, 1, 1])\n",
      "103 model.model.10.cv2.bn.weight torch.Size([256])\n",
      "104 model.model.10.cv2.bn.bias torch.Size([256])\n",
      "105 model.model.10.m.0.attn.qkv.conv.weight torch.Size([256, 128, 1, 1])\n",
      "106 model.model.10.m.0.attn.qkv.bn.weight torch.Size([256])\n",
      "107 model.model.10.m.0.attn.qkv.bn.bias torch.Size([256])\n",
      "108 model.model.10.m.0.attn.proj.conv.weight torch.Size([128, 128, 1, 1])\n",
      "109 model.model.10.m.0.attn.proj.bn.weight torch.Size([128])\n",
      "110 model.model.10.m.0.attn.proj.bn.bias torch.Size([128])\n",
      "111 model.model.10.m.0.attn.pe.conv.weight torch.Size([128, 1, 3, 3])\n",
      "112 model.model.10.m.0.attn.pe.bn.weight torch.Size([128])\n",
      "113 model.model.10.m.0.attn.pe.bn.bias torch.Size([128])\n",
      "114 model.model.10.m.0.ffn.0.conv.weight torch.Size([256, 128, 1, 1])\n",
      "115 model.model.10.m.0.ffn.0.bn.weight torch.Size([256])\n",
      "116 model.model.10.m.0.ffn.0.bn.bias torch.Size([256])\n",
      "117 model.model.10.m.0.ffn.1.conv.weight torch.Size([128, 256, 1, 1])\n",
      "118 model.model.10.m.0.ffn.1.bn.weight torch.Size([128])\n",
      "119 model.model.10.m.0.ffn.1.bn.bias torch.Size([128])\n",
      "120 model.model.13.cv1.conv.weight torch.Size([128, 384, 1, 1])\n",
      "121 model.model.13.cv1.bn.weight torch.Size([128])\n",
      "122 model.model.13.cv1.bn.bias torch.Size([128])\n",
      "123 model.model.13.cv2.conv.weight torch.Size([128, 192, 1, 1])\n",
      "124 model.model.13.cv2.bn.weight torch.Size([128])\n",
      "125 model.model.13.cv2.bn.bias torch.Size([128])\n",
      "126 model.model.13.m.0.cv1.conv.weight torch.Size([32, 64, 3, 3])\n",
      "127 model.model.13.m.0.cv1.bn.weight torch.Size([32])\n",
      "128 model.model.13.m.0.cv1.bn.bias torch.Size([32])\n",
      "129 model.model.13.m.0.cv2.conv.weight torch.Size([64, 32, 3, 3])\n",
      "130 model.model.13.m.0.cv2.bn.weight torch.Size([64])\n",
      "131 model.model.13.m.0.cv2.bn.bias torch.Size([64])\n",
      "132 model.model.16.cv1.conv.weight torch.Size([64, 256, 1, 1])\n",
      "133 model.model.16.cv1.bn.weight torch.Size([64])\n",
      "134 model.model.16.cv1.bn.bias torch.Size([64])\n",
      "135 model.model.16.cv2.conv.weight torch.Size([64, 96, 1, 1])\n",
      "136 model.model.16.cv2.bn.weight torch.Size([64])\n",
      "137 model.model.16.cv2.bn.bias torch.Size([64])\n",
      "138 model.model.16.m.0.cv1.conv.weight torch.Size([16, 32, 3, 3])\n",
      "139 model.model.16.m.0.cv1.bn.weight torch.Size([16])\n",
      "140 model.model.16.m.0.cv1.bn.bias torch.Size([16])\n",
      "141 model.model.16.m.0.cv2.conv.weight torch.Size([32, 16, 3, 3])\n",
      "142 model.model.16.m.0.cv2.bn.weight torch.Size([32])\n",
      "143 model.model.16.m.0.cv2.bn.bias torch.Size([32])\n",
      "144 model.model.17.conv.weight torch.Size([64, 64, 3, 3])\n",
      "145 model.model.17.bn.weight torch.Size([64])\n",
      "146 model.model.17.bn.bias torch.Size([64])\n",
      "147 model.model.19.cv1.conv.weight torch.Size([128, 192, 1, 1])\n",
      "148 model.model.19.cv1.bn.weight torch.Size([128])\n",
      "149 model.model.19.cv1.bn.bias torch.Size([128])\n",
      "150 model.model.19.cv2.conv.weight torch.Size([128, 192, 1, 1])\n",
      "151 model.model.19.cv2.bn.weight torch.Size([128])\n",
      "152 model.model.19.cv2.bn.bias torch.Size([128])\n",
      "153 model.model.19.m.0.cv1.conv.weight torch.Size([32, 64, 3, 3])\n",
      "154 model.model.19.m.0.cv1.bn.weight torch.Size([32])\n",
      "155 model.model.19.m.0.cv1.bn.bias torch.Size([32])\n",
      "156 model.model.19.m.0.cv2.conv.weight torch.Size([64, 32, 3, 3])\n",
      "157 model.model.19.m.0.cv2.bn.weight torch.Size([64])\n",
      "158 model.model.19.m.0.cv2.bn.bias torch.Size([64])\n",
      "159 model.model.20.conv.weight torch.Size([128, 128, 3, 3])\n",
      "160 model.model.20.bn.weight torch.Size([128])\n",
      "161 model.model.20.bn.bias torch.Size([128])\n",
      "162 model.model.22.cv1.conv.weight torch.Size([256, 384, 1, 1])\n",
      "163 model.model.22.cv1.bn.weight torch.Size([256])\n",
      "164 model.model.22.cv1.bn.bias torch.Size([256])\n",
      "165 model.model.22.cv2.conv.weight torch.Size([256, 384, 1, 1])\n",
      "166 model.model.22.cv2.bn.weight torch.Size([256])\n",
      "167 model.model.22.cv2.bn.bias torch.Size([256])\n",
      "168 model.model.22.m.0.cv1.conv.weight torch.Size([64, 128, 1, 1])\n",
      "169 model.model.22.m.0.cv1.bn.weight torch.Size([64])\n",
      "170 model.model.22.m.0.cv1.bn.bias torch.Size([64])\n",
      "171 model.model.22.m.0.cv2.conv.weight torch.Size([64, 128, 1, 1])\n",
      "172 model.model.22.m.0.cv2.bn.weight torch.Size([64])\n",
      "173 model.model.22.m.0.cv2.bn.bias torch.Size([64])\n",
      "174 model.model.22.m.0.cv3.conv.weight torch.Size([128, 128, 1, 1])\n",
      "175 model.model.22.m.0.cv3.bn.weight torch.Size([128])\n",
      "176 model.model.22.m.0.cv3.bn.bias torch.Size([128])\n",
      "177 model.model.22.m.0.m.0.cv1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "178 model.model.22.m.0.m.0.cv1.bn.weight torch.Size([64])\n",
      "179 model.model.22.m.0.m.0.cv1.bn.bias torch.Size([64])\n",
      "180 model.model.22.m.0.m.0.cv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "181 model.model.22.m.0.m.0.cv2.bn.weight torch.Size([64])\n",
      "182 model.model.22.m.0.m.0.cv2.bn.bias torch.Size([64])\n",
      "183 model.model.22.m.0.m.1.cv1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "184 model.model.22.m.0.m.1.cv1.bn.weight torch.Size([64])\n",
      "185 model.model.22.m.0.m.1.cv1.bn.bias torch.Size([64])\n",
      "186 model.model.22.m.0.m.1.cv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "187 model.model.22.m.0.m.1.cv2.bn.weight torch.Size([64])\n",
      "188 model.model.22.m.0.m.1.cv2.bn.bias torch.Size([64])\n",
      "189 model.model.23.cv2.0.0.conv.weight torch.Size([64, 64, 3, 3])\n",
      "190 model.model.23.cv2.0.0.bn.weight torch.Size([64])\n",
      "191 model.model.23.cv2.0.0.bn.bias torch.Size([64])\n",
      "192 model.model.23.cv2.0.1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "193 model.model.23.cv2.0.1.bn.weight torch.Size([64])\n",
      "194 model.model.23.cv2.0.1.bn.bias torch.Size([64])\n",
      "195 model.model.23.cv2.0.2.weight torch.Size([64, 64, 1, 1])\n",
      "196 model.model.23.cv2.0.2.bias torch.Size([64])\n",
      "197 model.model.23.cv2.1.0.conv.weight torch.Size([64, 128, 3, 3])\n",
      "198 model.model.23.cv2.1.0.bn.weight torch.Size([64])\n",
      "199 model.model.23.cv2.1.0.bn.bias torch.Size([64])\n",
      "200 model.model.23.cv2.1.1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "201 model.model.23.cv2.1.1.bn.weight torch.Size([64])\n",
      "202 model.model.23.cv2.1.1.bn.bias torch.Size([64])\n",
      "203 model.model.23.cv2.1.2.weight torch.Size([64, 64, 1, 1])\n",
      "204 model.model.23.cv2.1.2.bias torch.Size([64])\n",
      "205 model.model.23.cv2.2.0.conv.weight torch.Size([64, 256, 3, 3])\n",
      "206 model.model.23.cv2.2.0.bn.weight torch.Size([64])\n",
      "207 model.model.23.cv2.2.0.bn.bias torch.Size([64])\n",
      "208 model.model.23.cv2.2.1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "209 model.model.23.cv2.2.1.bn.weight torch.Size([64])\n",
      "210 model.model.23.cv2.2.1.bn.bias torch.Size([64])\n",
      "211 model.model.23.cv2.2.2.weight torch.Size([64, 64, 1, 1])\n",
      "212 model.model.23.cv2.2.2.bias torch.Size([64])\n",
      "213 model.model.23.cv3.0.0.0.conv.weight torch.Size([64, 1, 3, 3])\n",
      "214 model.model.23.cv3.0.0.0.bn.weight torch.Size([64])\n",
      "215 model.model.23.cv3.0.0.0.bn.bias torch.Size([64])\n",
      "216 model.model.23.cv3.0.0.1.conv.weight torch.Size([80, 64, 1, 1])\n",
      "217 model.model.23.cv3.0.0.1.bn.weight torch.Size([80])\n",
      "218 model.model.23.cv3.0.0.1.bn.bias torch.Size([80])\n",
      "219 model.model.23.cv3.0.1.0.conv.weight torch.Size([80, 1, 3, 3])\n",
      "220 model.model.23.cv3.0.1.0.bn.weight torch.Size([80])\n",
      "221 model.model.23.cv3.0.1.0.bn.bias torch.Size([80])\n",
      "222 model.model.23.cv3.0.1.1.conv.weight torch.Size([80, 80, 1, 1])\n",
      "223 model.model.23.cv3.0.1.1.bn.weight torch.Size([80])\n",
      "224 model.model.23.cv3.0.1.1.bn.bias torch.Size([80])\n",
      "225 model.model.23.cv3.0.2.weight torch.Size([80, 80, 1, 1])\n",
      "226 model.model.23.cv3.0.2.bias torch.Size([80])\n",
      "227 model.model.23.cv3.1.0.0.conv.weight torch.Size([128, 1, 3, 3])\n",
      "228 model.model.23.cv3.1.0.0.bn.weight torch.Size([128])\n",
      "229 model.model.23.cv3.1.0.0.bn.bias torch.Size([128])\n",
      "230 model.model.23.cv3.1.0.1.conv.weight torch.Size([80, 128, 1, 1])\n",
      "231 model.model.23.cv3.1.0.1.bn.weight torch.Size([80])\n",
      "232 model.model.23.cv3.1.0.1.bn.bias torch.Size([80])\n",
      "233 model.model.23.cv3.1.1.0.conv.weight torch.Size([80, 1, 3, 3])\n",
      "234 model.model.23.cv3.1.1.0.bn.weight torch.Size([80])\n",
      "235 model.model.23.cv3.1.1.0.bn.bias torch.Size([80])\n",
      "236 model.model.23.cv3.1.1.1.conv.weight torch.Size([80, 80, 1, 1])\n",
      "237 model.model.23.cv3.1.1.1.bn.weight torch.Size([80])\n",
      "238 model.model.23.cv3.1.1.1.bn.bias torch.Size([80])\n",
      "239 model.model.23.cv3.1.2.weight torch.Size([80, 80, 1, 1])\n",
      "240 model.model.23.cv3.1.2.bias torch.Size([80])\n",
      "241 model.model.23.cv3.2.0.0.conv.weight torch.Size([256, 1, 3, 3])\n",
      "242 model.model.23.cv3.2.0.0.bn.weight torch.Size([256])\n",
      "243 model.model.23.cv3.2.0.0.bn.bias torch.Size([256])\n",
      "244 model.model.23.cv3.2.0.1.conv.weight torch.Size([80, 256, 1, 1])\n",
      "245 model.model.23.cv3.2.0.1.bn.weight torch.Size([80])\n",
      "246 model.model.23.cv3.2.0.1.bn.bias torch.Size([80])\n",
      "247 model.model.23.cv3.2.1.0.conv.weight torch.Size([80, 1, 3, 3])\n",
      "248 model.model.23.cv3.2.1.0.bn.weight torch.Size([80])\n",
      "249 model.model.23.cv3.2.1.0.bn.bias torch.Size([80])\n",
      "250 model.model.23.cv3.2.1.1.conv.weight torch.Size([80, 80, 1, 1])\n",
      "251 model.model.23.cv3.2.1.1.bn.weight torch.Size([80])\n",
      "252 model.model.23.cv3.2.1.1.bn.bias torch.Size([80])\n",
      "253 model.model.23.cv3.2.2.weight torch.Size([80, 80, 1, 1])\n",
      "254 model.model.23.cv3.2.2.bias torch.Size([80])\n",
      "255 model.model.23.dfl.conv.weight torch.Size([1, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "for layer_nr, (name, param) in enumerate(model.named_parameters()):    \n",
    "    print(layer_nr, name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2896995",
   "metadata": {},
   "source": [
    "# Ein eigenen Objektdetektor aus Python heraus verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f734d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "model = YOLO(\"/home/juebrauer/link_to_vcd/10_datasets/58_hardhat_workers/runs/detect/train4/weights/best.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model(\"/home/juebrauer/link_to_vcd/10_datasets/59_hardhat_test_images/001.jpg\", verbose=False)  # predict on an image\n",
    "\n",
    "result = results[0]\n",
    "\n",
    "xywh = result.boxes.xywh  # center-x, center-y, width, height\n",
    "xywhn = result.boxes.xywhn  # normalized\n",
    "xyxy = result.boxes.xyxy  # top-left-x, top-left-y, bottom-right-x, bottom-right-y\n",
    "xyxyn = result.boxes.xyxyn  # normalized\n",
    "names = [result.names[cls.item()] for cls in result.boxes.cls.int()]  # class name of each box\n",
    "confs = result.boxes.conf  # confidence score of each box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ef861c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[553.7574,  77.9223, 116.0840, 155.8445],\n",
       "        [279.3323, 146.4477, 114.2601, 136.6698],\n",
       "        [130.0940, 131.5591, 144.5980, 165.1276]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a69d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9048, 0.2013, 0.1897, 0.4027],\n",
       "        [0.4564, 0.3784, 0.1867, 0.3532],\n",
       "        [0.2126, 0.3399, 0.2363, 0.4267]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xywhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900e7ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[495.7155,   0.0000, 611.7994, 155.8445],\n",
       "        [222.2022,  78.1128, 336.4624, 214.7826],\n",
       "        [ 57.7950,  48.9953, 202.3931, 214.1229]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ab5738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8100, 0.0000, 0.9997, 0.4027],\n",
       "        [0.3631, 0.2018, 0.5498, 0.5550],\n",
       "        [0.0944, 0.1266, 0.3307, 0.5533]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyxyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a8882a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helmet', 'helmet', 'helmet']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "325ce03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8842, 0.8757, 0.8689], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd713af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
