{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9a3e36",
   "metadata": {},
   "source": [
    "# Komplettes Skript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2aa7cbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n",
      "Nach conv1 hat Tensor die Shape: torch.Size([128, 32, 32, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m logits = model(imgs)\n\u001b[32m     75\u001b[39m loss = criterion(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m optimizer.step()\n\u001b[32m     79\u001b[39m running_loss += loss.item() * imgs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_teaching/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_teaching/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_teaching/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# mini_cifar10_cnn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---- Setup ----\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CIFAR-10: 32x32 RGB, Klassen=10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "# ---- Kleines CNN ----\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)   # 32x32 -> 32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 32x32 -> 32x32\n",
    "        self.pool  = nn.MaxPool2d(2, 2)               # 32x32 -> 16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # 16x16 -> 16x16\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)# 16x16 -> 16x16\n",
    "        self.head  = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),                  # -> 128x1x1\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(\"Nach conv1 hat Tensor die Shape:\", x.shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 1\n",
    "\n",
    "# ---- Training ----\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # ---- Evaluation ----\n",
    "    model.eval()\n",
    "    correct, total, test_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    test_loss /= total\n",
    "    test_acc = correct / total\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "          f\"test_loss={test_loss:.4f} acc={test_acc:.3f}\")\n",
    "\n",
    "# Optional: Modell speichern\n",
    "torch.save(model.state_dict(), \"small_cifar10_cnn.pt\")\n",
    "print(\"Saved to small_cifar10_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2da7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9040bf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ed4f7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68142877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d853fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cc64db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e618a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfbaae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "247a9ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil( len(train_ds) / train_loader.batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4ac5d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f93891aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    print(imgs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64893521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a1a397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d856e912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[  0., 255.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0., 255.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0., 255.,   0.],\n",
       "        [  0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy\n",
    "img = numpy.zeros(shape=(5,5,3))\n",
    "\n",
    "img[0,:,0] = 0\n",
    "img[0,:,1] = 255\n",
    "img[0,:,2] = 0\n",
    "\n",
    "img[:,2,1] = 255\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cec9d673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 9, 9, 9, 5, 5, 8, 5, 1, 3, 4, 1, 0, 9, 7, 9, 7, 3, 1, 8, 6, 7, 8, 4,\n",
       "        4, 8, 5, 8, 8, 1, 3, 5, 7, 7, 5, 5, 4, 2, 3, 4, 1, 8, 7, 5, 7, 8, 3, 7,\n",
       "        6, 6, 2, 1, 6, 0, 1, 0, 5, 9, 8, 7, 7, 2, 1, 1, 3, 0, 3, 6, 6, 7, 7, 7,\n",
       "        9, 0, 1, 9, 7, 6, 0, 5, 7, 1, 4, 6, 2, 6, 7, 4, 9, 1, 3, 1, 4, 6, 0, 9,\n",
       "        5, 7, 4, 4, 5, 8, 9, 2, 6, 3, 2, 9, 9, 3, 7, 0, 9, 3, 7, 4, 8, 9, 4, 7,\n",
       "        4, 8, 7, 0, 5, 9, 2, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1cba6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMGxJREFUeJzt3X1s3fV59/HPOcfnwc+O4yS2iZMm0BIoJLuXQWrRMkoykkxCUKIJ2koLHQLBEjTIuraZWihskxmVWtoqDX+MkVVqoGVqQHCvMAiNUbeELRlRSrtlJEub0MTOo5+OfZ5/9x8U3zMEuK7Eydc275d0pNi+cvn7ezqXj885H8eiKIoEAMB5Fg+9AADAhxMDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQRFXoBbxTpVLR4cOHVV9fr1gsFno5AACnKIo0ODio9vZ2xePv/Thnwg2gw4cPq6OjI/QyAABn6dChQ5o9e/Z7fv2cDaANGzboG9/4hnp6erRo0SJ997vf1ZVXXvmB/6++vl6S9OX1NymdSZq+V7GcMK9rZMRcKklqaMiYa+Nx3yO2Sr5ori0XfIeqmC+Za2urq129I+dZU5B9LUMDvgNUnbEf+5h8vYdOHnBUH3P1bmme4aqPSvZzKxcru3on0wVz7X+/8StX72LZvp0XXvgpV+9Y3J4iliv0u3oPD7vKVZWpNdcWsr7mpch+3uZytvvMtxXzFXNtS5P9vrBQKOr7m/7v6P35ezknA+iHP/yh1q1bp0cffVRLlizRI488ouXLl2vv3r2aOXPm+/7ft3/tls4klcmkTN8v4RhAFfv+liTzGqQzGECOXzGW475DFXc8vefZRsk/gDxrKeZ9d57ptGcA2QehJBVTng31PZ2ayfh2YlS0nyuR8zfXKccQT6WczUv2/ZJO+87DWMI+gKKY74655DsNlfSsvWj/wVOS4pH9vC1XfNupyH6HmEo5e0sf+DTKOXkRwje/+U3dfvvt+sIXvqBLL71Ujz76qGpqavT3f//35+LbAQAmoXEfQIVCQbt27dKyZcv+/zeJx7Vs2TJt3779XfX5fF4DAwNjbgCAqW/cB9Dx48dVLpc1a9asMZ+fNWuWenp63lXf1dWlxsbG0RsvQACAD4fg7wNav369+vv7R2+HDh0KvSQAwHkw7i9CaGlpUSKRUG9v75jP9/b2qrW19V316XRa6XR6vJcBAJjgxv0RUCqV0uLFi7V169bRz1UqFW3dulWdnZ3j/e0AAJPUOXkZ9rp167R69Wr93u/9nq688ko98sgjymaz+sIXvnAuvh0AYBI6JwPo5ptv1rFjx3Tfffepp6dHv/M7v6Pnn3/+XS9MAAB8eJ2zJIS1a9dq7dq1Z/z/q2ublKm2PTdU6Le/dNvxvitJUqVgf0daoWh/R7kkNdXV2Iudb9LL1zSYa+Mx304Zyg666hMZ+3N8sYTvHYCFYt5c2zDd/m51Saque+8IkXcaOu57d3vRea7Up9//Ddz/W27E1zsVt7/BsGWa71wpOd4AWtfgfBd/yf5G1FLZd1eXG7GfV5IkxzU0f+5HXK2zwyfMtaf6sq7eibj92qyusr8JOV9le3Yn+KvgAAAfTgwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEOcsiudsHT9ekPWvNFTK9oiIRMW3ycOOWJN4yb4OScql7PWpat/PCuWcfd1D+Zyr99FTp1z1dXX15tpExR6vIklR0r4PCyVfzE+iVLSvw3EOStJw3hfdUxnpN9fmVO3rnbDv8/pae8STJBWjEXPt0NC7/2Dl+4nK9rXU1vpimOprfMczFrfv80rZF2eUKNjPlQb5YrKqq+0RXyUlzLVRhSgeAMAExgACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAAQxYbPgSqWsEgljFlfZnlFUkzQGzP1WpcqeCeWd5rGSfd2ZuG/dgyPHzbWFfN7Vuz6TdNXHCo48vXTG1btStq8ld9J3hPpPZc21TdW+dbfVTXfVD52y54F5j8+prD2vrVKy7xNJqsnY72IqRfs6JKm+vsVcm0z78tfqMr5MwnJkv5/o7+9z9S71D5hrMwn7tSZJ8bh9v1Rn7LlxMeOlxiMgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQEzaK54K2JlVX26IfsoMlR+d61zoyaXvExoneE67ew0PGqCFJ5aI9ikWSqpO15trIESMiSTF7gtBbyvbjU7RmeLy9loq9d6zS5+p9QY19v1QV7HEpklQ4ZY9KkqRCsdpR64tjOTFiP7dmtjS6emeS9vrefud5KPt2NjjP2XhkvzYlKZm0n4cVXyqQso76KO67S6/LNJhrU46YrEpk2+E8AgIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEMWGz4JKVpJIVWxZcQ509J+tI75BrHYVh+4wezvry2hwxZsrIl5OVidv3SSle4+pdKviyxlIVe65WMnbS1TtdbT8+R/tGXL0bqpPm2lRds6t3/9Cgq75nJDLX5pzHpyB7HtipQd9dxmC/fZ+XEtN9vSP7BVTO+7LdEpHvZ/NkwnZfJUmK+3o3TGsy13rvg35z5JS5dmZzi7k2l7cdGx4BAQCCGPcB9PWvf12xWGzMbcGCBeP9bQAAk9w5+RXcxz/+cb300kv//5tUTdjf9AEAAjknk6Gqqkqtra3nojUAYIo4J88BvfHGG2pvb9f8+fP1+c9/XgcPHnzP2nw+r4GBgTE3AMDUN+4DaMmSJdq0aZOef/55bdy4UQcOHNCnPvUpDQ6e/lU/XV1damxsHL11dHSM95IAABPQuA+glStX6o/+6I+0cOFCLV++XP/0T/+kvr4+/ehHPzpt/fr169Xf3z96O3To0HgvCQAwAZ3zVwc0NTXpYx/7mPbt23far6fTaaXT6XO9DADABHPO3wc0NDSk/fv3q62t7Vx/KwDAJDLuA+iLX/yiuru79atf/Ur/+q//qs985jNKJBL67Gc/O97fCgAwiY37r+DefPNNffazn9WJEyc0Y8YMffKTn9SOHTs0Y8YMV5/e3kFl0nlTbTyZsDcuOfJvJA0O2WNNqjO+uBxF9qiXXN62L95WcdRWJX2nQdEZ9VIqOeKPYsddvXORPQKl5Px5a6hsP57FxDRX72K63lVf70ipqav4zsMoZo/iGYnskUCSVMzaI3Aq8vVOxO1neSVfdvXOlX2xTYrnzKWpVK2rdSZmf4oiSvquzaGcfZ8XivbaorF23AfQk08+Od4tAQBTEFlwAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgzvmfYzhTxdKI4glbbltV0Z6p1lRX7VpHLu7IP4p8OUyDA/Z8t4rzZ4VMwp4HVio4stokVcr23CtJqlTZc7UqkT07TJLyWfs+TKV8xz5VZd/nMTnyCCUlU776eMWee5aQLwsu5Tg+iZxv3blqe+5Z/+BJV+9+x/U2rc73J19KRd85Xi7ar6FU0p5fKEmFoj3HLhH3ZV3OnNFsrs071qGYrZZHQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAICZsFE9LbVqZjC2yIqrYo0diBV9URTFnj/vI2dNSJEkjRXvsTLnc7+odleyLSWdqXL3Tad/PLYm0PQInl3PEfUgqOo5nPO6MM0rZY5iiyLfufNl5ssTs53gm7YvLacjYo3hUybp6x5NzzbXl0jRX7+N9J+zriPmu+5qqjKs+n7cf/5EqX9xU74D9Pqh/xNf74/PtI2BGoz3ybCRnO195BAQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIYsJmwUWVhKKKLdOqULLnH1Uq9nwvSRrK58y1hbI9K0mSjp84Yq6dUTfg6l1ba8+yysWGXL1HCo2u+qq8PcesUrHl/70tmbCfwsXIl7+WM55/klQu+rLGlLDvE0nK1Npra2t8l3Wsyr5fGiJf78KQ/dyqSU939W5K15trywVfRloqk3bVxxz3K4W8L09vePikufZQr+8cH8j1mGvnXWA/PoW8Lb+OR0AAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAICZsFtxwuaxKqWyqLeQ9OVy+LLia2mpzbWXIln80qnDMXHrp78x0tW6+oMFc+9/7Drt6R8fsGWmSlMva90smNuLqnUnb6yuJGlfvXKHOXBuL+XIAU3Ffdlw8sl0LkqSKMwsubl975NzOdNJ+vZWTvhyzmox9O/sGfVmKMfmy4BJx+9qL+byrd3PGnhv464IvZ+7UgON4JgbNpcWiLXuPR0AAgCDcA+iVV17R9ddfr/b2dsViMT399NNjvh5Fke677z61tbWpurpay5Yt0xtvvDFe6wUATBHuAZTNZrVo0SJt2LDhtF9/+OGH9Z3vfEePPvqoXn31VdXW1mr58uXK5ex/1gAAMPW5nwNauXKlVq5cedqvRVGkRx55RF/96ld1ww03SJK+//3va9asWXr66ad1yy23nN1qAQBTxrg+B3TgwAH19PRo2bJlo59rbGzUkiVLtH379tP+n3w+r4GBgTE3AMDUN64DqKfnrb+uN2vWrDGfnzVr1ujX3qmrq0uNjY2jt46OjvFcEgBgggr+Krj169erv79/9Hbo0KHQSwIAnAfjOoBaW1slSb29vWM+39vbO/q1d0qn02poaBhzAwBMfeM6gObNm6fW1lZt3bp19HMDAwN69dVX1dnZOZ7fCgAwyblfBTc0NKR9+/aNfnzgwAHt3r1bzc3NmjNnju655x799V//tT760Y9q3rx5+trXvqb29nbdeOON47luAMAk5x5AO3fu1Kc//enRj9etWydJWr16tTZt2qQvfelLymazuuOOO9TX16dPfvKTev7555XJZFzfJ1coKooZIyisdZLiiZRrHcmYPUqkOuWL2JjeYl/3nEtmuHrX19vrD/yPPWJDkkaGfXE5jbX2+v9zQb2rd99xe8zPoYLvvWiJlD3+JpHwxRPFHeesJKUT9usnVrbFoLwtKtrP21iV75cmNTX2qJdSwRE3JKmUtq/FeWkqGfPtw0TCETlU8N3tzmqwH/v25hOu3sfK9kio1hb7fWfBeFm6B9A111yjKHrvnR2LxfTggw/qwQcf9LYGAHyIBH8VHADgw4kBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACMIdxXO+VNemlMnYsocG+u25TSPD9uwjSUom7JldpbwvyyqWtq/72NHjrt5HD9hzzwaO+9ZdiWVd9XMX2PPaLp95gav3//2VPSfr6MB+V++O2fbjk0ykXb0V82XeRY5osijhyzGrOA5/qtqX6Viu2GujAd95Fcl+bVY7D0+xMOyqr62zZwGWYo6dIqlctvduqPc9pkjG7Vl9jbX2/Z2vstXyCAgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEMSEjeIZHBxRoWCLzSmV7BERw8O+mZst2qN7YjFfBEq1Y++/vut/XL0rajLXFkZqXL2nN/u2s/1C+1oO7+1z9R4a6DfXts20R5pIUstM+34p5yJX79oGX6TNQJ89zqhQtscwSVJ93XR7sT25RZI0NGSPnYnHfPskVrJvZ13GF8GV8OQTSUo40nVSSd95WHQsJRGzxZeNriWy75cBR1RSoWC7j+AREAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCICZsFN1KUysbxmCjbs+Aqzoyniuz16ZphV+/6VJO5NlYZcfWOVVWba+vqfQFfbRekXfXTpn3MXFv9UV+O2e9l+8y1J+qdP29las2lI8q7WkexId9SauxZc+lKi6t3ssqeHxbFHKFnkmrS9t4jCd85HiUHzLW11b7zqpC3Z+9J0kjefm6VI18WXDptv5ZTcV8WXDmy37/lCvZ9WCQLDgAwkTGAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQUzYKJ5UKq50yhZZMThgjwdJpXxRPFFp0FybqJxw9fbEYESlkqt3Q7W9vrmtwdW75SO+OJZErf3nnJr62a7e0aEj5tpkjS/+JkrZ153I+c6rUsV3PGNJe8RKJjHd1bsqZY+yimSLWBmtr/Sba8uFk67ehaz9PJxW44uPaprmu2scGLTHCPUN+a4fj4TzMUUqbo8FGi7ba6OybRt5BAQACIIBBAAIwj2AXnnlFV1//fVqb29XLBbT008/Pebrt956q2Kx2JjbihUrxmu9AIApwj2AstmsFi1apA0bNrxnzYoVK3TkyJHR2xNPPHFWiwQATD3uFyGsXLlSK1eufN+adDqt1tbWM14UAGDqOyfPAW3btk0zZ87UxRdfrLvuuksnTrz3q8Py+bwGBgbG3AAAU9+4D6AVK1bo+9//vrZu3aq//du/VXd3t1auXKly+fQvU+3q6lJjY+PoraOjY7yXBACYgMb9fUC33HLL6L8vv/xyLVy4UBdeeKG2bdumpUuXvqt+/fr1Wrdu3ejHAwMDDCEA+BA45y/Dnj9/vlpaWrRv377Tfj2dTquhoWHMDQAw9Z3zAfTmm2/qxIkTamtrO9ffCgAwibh/BTc0NDTm0cyBAwe0e/duNTc3q7m5WQ888IBWrVql1tZW7d+/X1/60pd00UUXafny5eO6cADA5OYeQDt37tSnP/3p0Y/ffv5m9erV2rhxo/bs2aN/+Id/UF9fn9rb23Xdddfpr/7qr5RO+7KYYiUplohMtYWCPYdpznTfOo6dsvfOFYZdvRMZe35YdbVtX7xt9txmc+2sj9hrJSmqd2beJez7ZTBKunpnPmJ/ZF2R7xWW5YQ9f62U9R2fXNa3lmTavpaRkZyrd6JsvyZikS/Drpyzb+dgnz13UZJ6jtl/gdMyy5uP59vOxlp7nl655LvbLToyI+Mx33lYFdnX3eRYdsEYd+ceQNdcc42i6L038oUXXvC2BAB8CJEFBwAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIYtz/HtB4qZQiVYxZcA1xe1ZSc5Uv46mnaN9F8bivd326YK6d27HA1XvewsvNtemmHlfv4ZEaV30pZ8/TKzmzxmKN9tqqQsbVu1IxBlpJyjQ7f5aLJVzlcUd9VPZd1smMfb8Uhu3HUpLyQ/ZzvJD3ZSmWEvYcs5PFoqv3UJ/veNYnqs21pYp93ZI0NGLfh5mML+syk7TX109vMtfmcrY18wgIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABDEhI3iiSeqFE8kTbV1jvSJioZc6yiUbGuQpHTSHgkkSfWZnLm2o2OWq/cFF11srs2VfLEw6VS/q74cc8TrxHynZCFpj4aJSr6ol7gjuqdQ9MWrJBO1zvomc22q7DsPazL2n0OHCr6fWfuz9rVkjw+6eqtsj7/5zaE+V+uaTIOr/pRssWGSlM3bI54kaSRn733JfEc2laRL5rSba6Mq+/4eHsmb6ngEBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAhiwmbBlcsFlcu2DKSquD1baTjy5Z7NaJlhrj1++L9dvSvD9pysZNVvXL1VsdcnY/a8O0kqxu3ZVJIUle3HJ591ZvU56kf67Nl7khRzZMFVZae5eg+fHHDVnxyw59hVp+2ZXZKUaLBn9Q2d6HH1PtZjzw08NVR09U6lmsy1mZjvZ+1EzHeuVGTf5/lcn6t3LD7dXFusOIIxJeVytsw2SWpotN9PlBO2c4pHQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAICZsFE8yVlQyZqutq7bH6+Qr9ugJScr22yNtRuxpKZKkXx+w117YccLVu7F+n7k2UVvv6p1wxpqoYj8+pT57PJEk1eUb7L2P+SKHhor2uJxKyRfdcvx/fMfzRL89Fmj23CZX75rkSXPt4cO+7ezLtZhrm1vtx1KSajN15tpCzB4HJUnF8oirviFlP2+b633n4dGcvf6kb9k60GM/Dy+oFMy1IzlbLY+AAABBuAZQV1eXrrjiCtXX12vmzJm68cYbtXfv3jE1uVxOa9as0fTp01VXV6dVq1apt7d3XBcNAJj8XAOou7tba9as0Y4dO/Tiiy+qWCzquuuuUzabHa2599579eyzz+qpp55Sd3e3Dh8+rJtuumncFw4AmNxczwE9//zzYz7etGmTZs6cqV27dunqq69Wf3+/HnvsMW3evFnXXnutJOnxxx/XJZdcoh07dugTn/jE+K0cADCpndVzQP39b/2tj+bmZknSrl27VCwWtWzZstGaBQsWaM6cOdq+fftpe+TzeQ0MDIy5AQCmvjMeQJVKRffcc4+uuuoqXXbZZZKknp4epVIpNTU1jamdNWuWenpO/4esurq61NjYOHrr6Og40yUBACaRMx5Aa9as0euvv64nn3zyrBawfv169ff3j94OHTp0Vv0AAJPDGb0PaO3atXruuef0yiuvaPbs2aOfb21tVaFQUF9f35hHQb29vWptbT1tr3Q6rXTa92dkAQCTn+sRUBRFWrt2rbZs2aKXX35Z8+bNG/P1xYsXK5lMauvWraOf27t3rw4ePKjOzs7xWTEAYEpwPQJas2aNNm/erGeeeUb19fWjz+s0NjaqurpajY2Nuu2227Ru3To1NzeroaFBd999tzo7O3kFHABgDNcA2rhxoyTpmmuuGfP5xx9/XLfeeqsk6Vvf+pbi8bhWrVqlfD6v5cuX63vf+964LBYAMHW4BlAURR9Yk8lktGHDBm3YsOGMFyVJmWRFmaQtv6mqqmTuW8z5sqyK2X5zbaloDK/7rd8M2DOejh63515JUsNBe+ZdsewLsZs+/SOu+ljCnsMVZX2vi3nzmP34NNb59uH06kFz7ZFDR129KwV7Pp4kNTfUmGvjMfs+kaQ3j9rP28FSm6t3ptq+7trqoqt3Y5O9dzbv2ydDQ8dd9fGY/T4oEfmy4BqS9hzAoeyQq3dNxb7uE9lqc20ub+tLFhwAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIIgz+nMM50OqukaptC2yIpb44Iigt9UkfH/6obnZHt9SKftifo4O2SNQfvUbe5yNJI1Usubaof5Trt6z232RNtnsm+baqkTB1bv/xAlz7YKPzXH1bpvVZK5NRs2u3uXIHoEiSfVpewxKKuY7x0+V7fUNLefuLiNW8sUTRbJfb6mUPZpKkhoafXE5IwX7tRyr+H7ub3WsJT/siwNLyn69ZYv22lzRFqvEIyAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEBM2C65QKZszkxoyGXPfoRHfzD0yPGyurau2Z9JJ0vR6e++9e/a4eu/ZnTLX1jU2unrv/5/jrnqVh8ylVVW+U7Ku3l5f+O+Trt69/fZsssbaGlfvTNqe1SdJJwft51ZrY5Or96xpZXPt4Ii9VpIKcfvxScZ812asZM8BlGzZZKNrSda76iPPOV7jy7xLJe311U2+6ydXsN8HDY3YtzGft2Ud8ggIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABDExI3iiYqKGdNHBnP2OVqIql3rSDTUmmv7jh1w9U6m7PEgDS0xV+/hsr13TaMvXiU7eMxVn+uzxXJIUqHk2850Lm2uPXTKt51Nxwrm2umNvvOqpdEX9VJXa49WyudOuXqnHfFHUdl3fHKFnH0d1gv+t+oS9ut+xBjr9bZc3hfdk47scTnxKl8Uj2L28zBK+s7xRCxprq2q2M/BsjH6iEdAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAmbBZcLFatWMyWPXRyxJ431VDvm7lz2qeZa39T9uV7DR0bMNema3zrbsrY86Ma6n25V43TMq76UzVD5tpk2p7tJkly5GoVc76crEgnzbVHTvgy0k72N7jqP9rWbK7tmH2Bq/fRgYq5Nlux10rSSMVxbhV9WXC5YtZce6r/hKt3Il3nqq/K2M/bgnxZcC2NNebakcF+V+8TA/a15PP2vMNCwXat8QgIABCEawB1dXXpiiuuUH19vWbOnKkbb7xRe/fuHVNzzTXXKBaLjbndeeed47poAMDk5xpA3d3dWrNmjXbs2KEXX3xRxWJR1113nbLZsQ+Fb7/9dh05cmT09vDDD4/rogEAk5/rOaDnn39+zMebNm3SzJkztWvXLl199dWjn6+pqVFra+v4rBAAMCWd1XNA/f1vPeHV3Dz2CdIf/OAHamlp0WWXXab169dreHj4PXvk83kNDAyMuQEApr4zfhVcpVLRPffco6uuukqXXXbZ6Oc/97nPae7cuWpvb9eePXv05S9/WXv37tWPf/zj0/bp6urSAw88cKbLAABMUmc8gNasWaPXX39dP/vZz8Z8/o477hj99+WXX662tjYtXbpU+/fv14UXXviuPuvXr9e6detGPx4YGFBHR8eZLgsAMEmc0QBau3atnnvuOb3yyiuaPXv2+9YuWbJEkrRv377TDqB0Oq20970fAIBJzzWAoijS3XffrS1btmjbtm2aN2/eB/6f3bt3S5La2trOaIEAgKnJNYDWrFmjzZs365lnnlF9fb16enokSY2Njaqurtb+/fu1efNm/eEf/qGmT5+uPXv26N5779XVV1+thQsXnpMNAABMTq4BtHHjRklvvdn0f3v88cd16623KpVK6aWXXtIjjzyibDarjo4OrVq1Sl/96lfHbcEAgKnB/Su499PR0aHu7u6zWtDbmlOSNV5pWPYMqaqiPTdOkgaH7FlWtQ2+LLh8nz2H6USPL6+todm+T0bk2ydRzJd7VlNtz46rlH05WeVhe+bdjKb3f77ynRLxvLn22PHjrt6puC3n8G01jui4nHx5YEOOjK+hiu/YlxP2u5gjg72u3tHgYXuxc911tb7jE0/ZM/LKJWeeXs5eHxV8eXqFgj2n8VS/PXuvWCyZ6siCAwAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEccZ/D+hcu7A9rZpqWxxG2bEZhYIvkiM3bI/BGBjyRdrU1dgjamqby67e1Y5YoLLzx5CRsi1m422VuP0bDGdHXL2LOXv0SLKmxtV7Qfs0c21s2LfuyLnPM432iKJTOXs8kSSdyNqviZG4PbZHkkZKg+ba4dwxV++47NdmJmO/1iRpoOA7x8tJ+3mYsWaM/daxfvta0klfHFhLi/2aiKr6zLXW+1keAQEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCmLBZcKeyJeUqtvmYjNtzmGJVvplbXW3Pm6qunu3qfUHDgLm2qnrI1btcVWuuHTxlz+uSpOyAL/NuJGY/zYb7fMdneKBork03N7l6K2HPVOuY6cuZK9iXLUnKH7OfK/t+47usD52yn1v1zb6ssVTKfq60ttW5epejC8y12eF+V+/skP26l6T+o/ba1hm+LLhYlT1/L17lOw/T9faMvLYa+zryOdsJziMgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQEzaKpzoVV03KNh8HR+xRIrkhe2yPJFUcuyiqirl6V+XtcTmlk32u3uXopLk2nfJFoGRqffXVMXvUz/SkPRpEkioN9miYdH3K1Xs4b4/iians6l1J+OqHR+znbaaSdPVuyth/Dq1J+WJkptfbz/GM7/Aon2wx15byCVfvRMYXTzVYGjHXDmd9x76x0b7PYzHffZBkr8/n7PeF+bztfOUREAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCICZsF1zIzo9oaWzhUY96RZzRSca2jFNlztYaGc67eUZU9xywXm+7qncvZ15It+noPRr48sMFee65WTL4suGS60d7bcZ5IUqVi384aTXP1rpIva6xSZb9U51zQ5Oo9U0VzbSzh24d1Vfbcs6Ijl0ySRmTP6qvK+HoPFXz11Q0N5tqS8/opl+zbWRi2H0tJqp9mz+pLVdvvC6MYWXAAgAnMNYA2btyohQsXqqGhQQ0NDers7NRPfvKT0a/ncjmtWbNG06dPV11dnVatWqXe3t5xXzQAYPJzDaDZs2froYce0q5du7Rz505de+21uuGGG/SLX/xCknTvvffq2Wef1VNPPaXu7m4dPnxYN9100zlZOABgcnM9B3T99deP+fhv/uZvtHHjRu3YsUOzZ8/WY489ps2bN+vaa6+VJD3++OO65JJLtGPHDn3iE58Yv1UDACa9M34OqFwu68knn1Q2m1VnZ6d27dqlYrGoZcuWjdYsWLBAc+bM0fbt29+zTz6f18DAwJgbAGDqcw+gn//856qrq1M6ndadd96pLVu26NJLL1VPT49SqZSamprG1M+aNUs9PT3v2a+rq0uNjY2jt46ODvdGAAAmH/cAuvjii7V79269+uqruuuuu7R69Wr98pe/POMFrF+/Xv39/aO3Q4cOnXEvAMDk4X4fUCqV0kUXXSRJWrx4sf793/9d3/72t3XzzTerUCior69vzKOg3t5etba2vme/dDqtdNr3ungAwOR31u8DqlQqyufzWrx4sZLJpLZu3Tr6tb179+rgwYPq7Ow8228DAJhiXI+A1q9fr5UrV2rOnDkaHBzU5s2btW3bNr3wwgtqbGzUbbfdpnXr1qm5uVkNDQ26++671dnZySvgAADv4hpAR48e1R//8R/ryJEjamxs1MKFC/XCCy/oD/7gDyRJ3/rWtxSPx7Vq1Srl83ktX75c3/ve985oYSOlomIlaxxGwtw3kfH91jFWsUf31MR88R2puD2+I9bo+zVlqTJirs2Xnb8Cjdkikt4WTf+4uXYo54tKGhq2RX5I0qlBeyyMJI3k7OfVUDTT1btU9EUOqdhnLq1v8J3j1fX2+oQjtkeSCpH9+KjsqJWUqbbXJ2p9vWvTda76WJU9EqpUcrWWZxcODg67eldOvGmuzSQd929523niOlMfe+yx9/16JpPRhg0btGHDBk9bAMCHEFlwAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAINxp2Oda9NvcieGRgv0/lR0REZEvLqfiiOLJFx1rllSK29cSq/giUEqO+nzZ+XOIbxcqMsZySNJI3hfFk8vZc0ryeV8UT77g2Oe+pBeVis48lqJ97am8r7cnWSkhX+/I8zNu2XfslbQfn6J3f/tOFcVK9rWUnL09UTx5x7Um+e4Oqyr24vxvz8HoAxYfiz6o4jx78803+aN0ADAFHDp0SLNnz37Pr0+4AVSpVHT48GHV19cr9r/CPQcGBtTR0aFDhw6pocEe4jnZsJ1Tx4dhGyW2c6oZj+2MokiDg4Nqb29XPP7ej4In3K/g4vH4+07MhoaGKX3w38Z2Th0fhm2U2M6p5my3s7HxgxPCeRECACAIBhAAIIhJM4DS6bTuv/9+pdPOP542ybCdU8eHYRsltnOqOZ/bOeFehAAA+HCYNI+AAABTCwMIABAEAwgAEAQDCAAQxKQZQBs2bNBHPvIRZTIZLVmyRP/2b/8Weknj6utf/7pisdiY24IFC0Iv66y88soruv7669Xe3q5YLKann356zNejKNJ9992ntrY2VVdXa9myZXrjjTfCLPYsfNB23nrrre86titWrAiz2DPU1dWlK664QvX19Zo5c6ZuvPFG7d27d0xNLpfTmjVrNH36dNXV1WnVqlXq7e0NtOIzY9nOa6655l3H88477wy04jOzceNGLVy4cPTNpp2dnfrJT34y+vXzdSwnxQD64Q9/qHXr1un+++/Xf/zHf2jRokVavny5jh49Gnpp4+rjH/+4jhw5Mnr72c9+FnpJZyWbzWrRokXasGHDab/+8MMP6zvf+Y4effRRvfrqq6qtrdXy5cuVy+XO80rPzgdtpyStWLFizLF94oknzuMKz153d7fWrFmjHTt26MUXX1SxWNR1112nbDY7WnPvvffq2Wef1VNPPaXu7m4dPnxYN910U8BV+1m2U5Juv/32Mcfz4YcfDrTiMzN79mw99NBD2rVrl3bu3Klrr71WN9xwg37xi19IOo/HMpoErrzyymjNmjWjH5fL5ai9vT3q6uoKuKrxdf/990eLFi0KvYxzRlK0ZcuW0Y8rlUrU2toafeMb3xj9XF9fX5ROp6MnnngiwArHxzu3M4qiaPXq1dENN9wQZD3nytGjRyNJUXd3dxRFbx27ZDIZPfXUU6M1//mf/xlJirZv3x5qmWftndsZRVH0+7//+9Gf/dmfhVvUOTJt2rTo7/7u787rsZzwj4AKhYJ27dqlZcuWjX4uHo9r2bJl2r59e8CVjb833nhD7e3tmj9/vj7/+c/r4MGDoZd0zhw4cEA9PT1jjmtjY6OWLFky5Y6rJG3btk0zZ87UxRdfrLvuuksnTpwIvaSz0t/fL0lqbm6WJO3atUvFYnHM8VywYIHmzJkzqY/nO7fzbT/4wQ/U0tKiyy67TOvXr9fw8HCI5Y2LcrmsJ598UtlsVp2dnef1WE64MNJ3On78uMrlsmbNmjXm87NmzdJ//dd/BVrV+FuyZIk2bdqkiy++WEeOHNEDDzygT33qU3r99ddVX18fennjrqenR5JOe1zf/tpUsWLFCt10002aN2+e9u/fr7/8y7/UypUrtX37diUSidDLc6tUKrrnnnt01VVX6bLLLpP01vFMpVJqamoaUzuZj+fptlOSPve5z2nu3Llqb2/Xnj179OUvf1l79+7Vj3/844Cr9fv5z3+uzs5O5XI51dXVacuWLbr00ku1e/fu83YsJ/wA+rBYuXLl6L8XLlyoJUuWaO7cufrRj36k2267LeDKcLZuueWW0X9ffvnlWrhwoS688EJt27ZNS5cuDbiyM7NmzRq9/vrrk/45yg/yXtt5xx13jP778ssvV1tbm5YuXar9+/frwgsvPN/LPGMXX3yxdu/erf7+fv3jP/6jVq9ere7u7vO6hgn/K7iWlhYlEol3vQKjt7dXra2tgVZ17jU1NeljH/uY9u3bF3op58Tbx+7Ddlwlaf78+WppaZmUx3bt2rV67rnn9NOf/nTMn01pbW1VoVBQX1/fmPrJejzfaztPZ8mSJZI06Y5nKpXSRRddpMWLF6urq0uLFi3St7/97fN6LCf8AEqlUlq8eLG2bt06+rlKpaKtW7eqs7Mz4MrOraGhIe3fv19tbW2hl3JOzJs3T62trWOO68DAgF599dUpfVylt/7q74kTJybVsY2iSGvXrtWWLVv08ssva968eWO+vnjxYiWTyTHHc+/evTp48OCkOp4ftJ2ns3v3bkmaVMfzdCqVivL5/Pk9luP6koZz5Mknn4zS6XS0adOm6Je//GV0xx13RE1NTVFPT0/opY2bP//zP4+2bdsWHThwIPqXf/mXaNmyZVFLS0t09OjR0Es7Y4ODg9Frr70Wvfbaa5Gk6Jvf/Gb02muvRb/+9a+jKIqihx56KGpqaoqeeeaZaM+ePdENN9wQzZs3LxoZGQm8cp/3287BwcHoi1/8YrR9+/bowIED0UsvvRT97u/+bvTRj340yuVyoZdudtddd0WNjY3Rtm3boiNHjozehoeHR2vuvPPOaM6cOdHLL78c7dy5M+rs7Iw6OzsDrtrvg7Zz37590YMPPhjt3LkzOnDgQPTMM89E8+fPj66++urAK/f5yle+EnV3d0cHDhyI9uzZE33lK1+JYrFY9M///M9RFJ2/YzkpBlAURdF3v/vdaM6cOVEqlYquvPLKaMeOHaGXNK5uvvnmqK2tLUqlUtEFF1wQ3XzzzdG+fftCL+us/PSnP40kveu2evXqKIreein21772tWjWrFlROp2Oli5dGu3duzfsos/A+23n8PBwdN1110UzZsyIkslkNHfu3Oj222+fdD88nW77JEWPP/74aM3IyEj0p3/6p9G0adOimpqa6DOf+Ux05MiRcIs+Ax+0nQcPHoyuvvrqqLm5OUqn09FFF10U/cVf/EXU398fduFOf/InfxLNnTs3SqVS0YwZM6KlS5eODp8oOn/Hkj/HAAAIYsI/BwQAmJoYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAg/h96QE3FAcUiEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5255, 0.5373, 0.3647],\n",
       "         [0.5059, 0.5255, 0.3765],\n",
       "         [0.4667, 0.4863, 0.3765],\n",
       "         ...,\n",
       "         [0.3529, 0.3882, 0.3098],\n",
       "         [0.3333, 0.3608, 0.3020],\n",
       "         [0.4157, 0.4235, 0.3333]],\n",
       "\n",
       "        [[0.5608, 0.5843, 0.4314],\n",
       "         [0.5020, 0.5255, 0.3765],\n",
       "         [0.4627, 0.4784, 0.3765],\n",
       "         ...,\n",
       "         [0.4275, 0.4431, 0.3333],\n",
       "         [0.4392, 0.4510, 0.3608],\n",
       "         [0.4745, 0.4667, 0.3647]],\n",
       "\n",
       "        [[0.4784, 0.4980, 0.4039],\n",
       "         [0.4706, 0.4863, 0.3804],\n",
       "         [0.4745, 0.4706, 0.3961],\n",
       "         ...,\n",
       "         [0.4824, 0.4431, 0.2824],\n",
       "         [0.5176, 0.4863, 0.3569],\n",
       "         [0.5020, 0.4863, 0.3725]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6667, 0.5961, 0.3843],\n",
       "         [0.6157, 0.5608, 0.3373],\n",
       "         [0.5843, 0.5333, 0.3333],\n",
       "         ...,\n",
       "         [0.4667, 0.4706, 0.3569],\n",
       "         [0.5098, 0.5216, 0.3882],\n",
       "         [0.4706, 0.4784, 0.3490]],\n",
       "\n",
       "        [[0.7490, 0.6863, 0.4157],\n",
       "         [0.7255, 0.6588, 0.3882],\n",
       "         [0.6980, 0.6235, 0.3804],\n",
       "         ...,\n",
       "         [0.4235, 0.4314, 0.3216],\n",
       "         [0.5176, 0.5373, 0.4000],\n",
       "         [0.4941, 0.5137, 0.3686]],\n",
       "\n",
       "        [[0.7098, 0.6706, 0.3961],\n",
       "         [0.7098, 0.6588, 0.4000],\n",
       "         [0.7098, 0.6549, 0.4039],\n",
       "         ...,\n",
       "         [0.4824, 0.4314, 0.2902],\n",
       "         [0.5098, 0.4627, 0.2902],\n",
       "         [0.5255, 0.4824, 0.2941]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imgs[-1,:,:,:].permute(1,2,0)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a69cd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7267376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.0625"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000 / 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2ab91",
   "metadata": {},
   "source": [
    "Variante 2 mit Zeitmessung, adaptiver Anzahl an Epochen, Modellspeicherung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac203b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.8271 acc=0.297 (train_time=6.54s) | val_loss=1.6219 acc=0.382 (eval_time=0.82s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=1.6219)\n",
      "Epoch 02 | train_loss=1.5290 acc=0.428 (train_time=6.58s) | val_loss=1.3981 acc=0.491 (eval_time=0.81s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=1.3981)\n",
      "Epoch 03 | train_loss=1.3463 acc=0.511 (train_time=6.54s) | val_loss=1.2840 acc=0.545 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=1.2840)\n",
      "Epoch 04 | train_loss=1.2110 acc=0.563 (train_time=6.59s) | val_loss=1.1491 acc=0.585 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=1.1491)\n",
      "Epoch 05 | train_loss=1.1203 acc=0.600 (train_time=6.55s) | val_loss=1.1322 acc=0.597 (eval_time=0.80s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=1.1322)\n",
      "Epoch 06 | train_loss=1.0569 acc=0.624 (train_time=6.55s) | val_loss=1.0188 acc=0.640 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=1.0188)\n",
      "Epoch 07 | train_loss=1.0026 acc=0.645 (train_time=6.63s) | val_loss=0.9886 acc=0.653 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.9886)\n",
      "Epoch 08 | train_loss=0.9589 acc=0.660 (train_time=6.61s) | val_loss=1.0598 acc=0.621 (eval_time=0.79s)\n",
      "Epoch 09 | train_loss=0.9211 acc=0.674 (train_time=6.58s) | val_loss=1.0020 acc=0.644 (eval_time=0.81s)\n",
      "Epoch 10 | train_loss=0.8766 acc=0.691 (train_time=6.61s) | val_loss=0.8728 acc=0.697 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.8728)\n",
      "Epoch 11 | train_loss=0.8435 acc=0.703 (train_time=6.60s) | val_loss=0.8708 acc=0.697 (eval_time=0.82s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.8708)\n",
      "Epoch 12 | train_loss=0.8141 acc=0.716 (train_time=6.59s) | val_loss=0.8960 acc=0.688 (eval_time=0.89s)\n",
      "Epoch 13 | train_loss=0.7939 acc=0.721 (train_time=6.61s) | val_loss=0.8557 acc=0.705 (eval_time=0.80s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.8557)\n",
      "Epoch 14 | train_loss=0.7588 acc=0.735 (train_time=6.60s) | val_loss=0.8138 acc=0.725 (eval_time=0.79s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.8138)\n",
      "Epoch 15 | train_loss=0.7342 acc=0.741 (train_time=6.48s) | val_loss=0.7478 acc=0.742 (eval_time=0.84s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.7478)\n",
      "Epoch 16 | train_loss=0.7141 acc=0.750 (train_time=6.50s) | val_loss=0.7499 acc=0.742 (eval_time=0.81s)\n",
      "Epoch 17 | train_loss=0.6971 acc=0.756 (train_time=6.52s) | val_loss=0.7534 acc=0.741 (eval_time=0.80s)\n",
      "Epoch 18 | train_loss=0.6821 acc=0.761 (train_time=6.51s) | val_loss=0.7339 acc=0.750 (eval_time=0.82s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.7339)\n",
      "Epoch 19 | train_loss=0.6561 acc=0.772 (train_time=6.57s) | val_loss=0.7853 acc=0.743 (eval_time=0.82s)\n",
      "Epoch 20 | train_loss=0.6532 acc=0.772 (train_time=6.47s) | val_loss=0.6959 acc=0.767 (eval_time=0.80s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.6959)\n",
      "Epoch 21 | train_loss=0.6242 acc=0.783 (train_time=6.58s) | val_loss=0.7358 acc=0.749 (eval_time=0.81s)\n",
      "Epoch 22 | train_loss=0.6125 acc=0.788 (train_time=6.49s) | val_loss=0.7688 acc=0.756 (eval_time=0.80s)\n",
      "Epoch 23 | train_loss=0.5997 acc=0.792 (train_time=6.57s) | val_loss=0.6535 acc=0.783 (eval_time=0.86s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.6535)\n",
      "Epoch 24 | train_loss=0.5845 acc=0.799 (train_time=6.49s) | val_loss=0.6798 acc=0.778 (eval_time=0.82s)\n",
      "Epoch 25 | train_loss=0.5702 acc=0.803 (train_time=6.60s) | val_loss=0.7004 acc=0.768 (eval_time=0.83s)\n",
      "Epoch 26 | train_loss=0.5609 acc=0.806 (train_time=6.68s) | val_loss=0.6508 acc=0.784 (eval_time=0.82s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.6508)\n",
      "Epoch 27 | train_loss=0.5476 acc=0.811 (train_time=6.55s) | val_loss=0.6386 acc=0.785 (eval_time=0.80s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.6386)\n",
      "Epoch 28 | train_loss=0.5362 acc=0.814 (train_time=6.60s) | val_loss=0.6283 acc=0.792 (eval_time=0.80s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.6283)\n",
      "Epoch 29 | train_loss=0.5283 acc=0.818 (train_time=6.55s) | val_loss=0.6040 acc=0.799 (eval_time=0.82s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.6040)\n",
      "Epoch 30 | train_loss=0.5142 acc=0.822 (train_time=6.63s) | val_loss=0.6370 acc=0.794 (eval_time=0.84s)\n",
      "Epoch 31 | train_loss=0.5117 acc=0.823 (train_time=6.60s) | val_loss=0.5965 acc=0.797 (eval_time=0.85s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.5965)\n",
      "Epoch 32 | train_loss=0.5013 acc=0.828 (train_time=6.61s) | val_loss=0.6455 acc=0.790 (eval_time=0.82s)\n",
      "Epoch 33 | train_loss=0.4905 acc=0.831 (train_time=6.55s) | val_loss=0.5943 acc=0.808 (eval_time=0.81s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.5943)\n",
      "Epoch 34 | train_loss=0.4834 acc=0.833 (train_time=6.50s) | val_loss=0.6031 acc=0.803 (eval_time=0.82s)\n",
      "Epoch 35 | train_loss=0.4748 acc=0.836 (train_time=6.52s) | val_loss=0.6087 acc=0.803 (eval_time=0.82s)\n",
      "Epoch 36 | train_loss=0.4688 acc=0.838 (train_time=6.54s) | val_loss=0.5809 acc=0.807 (eval_time=0.84s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.5809)\n",
      "Epoch 37 | train_loss=0.4564 acc=0.842 (train_time=6.60s) | val_loss=0.5883 acc=0.814 (eval_time=0.82s)\n",
      "Epoch 38 | train_loss=0.4543 acc=0.843 (train_time=6.62s) | val_loss=0.5635 acc=0.813 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.5635)\n",
      "Epoch 39 | train_loss=0.4424 acc=0.847 (train_time=6.57s) | val_loss=0.5598 acc=0.811 (eval_time=0.83s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.5598)\n",
      "Epoch 40 | train_loss=0.4363 acc=0.849 (train_time=6.68s) | val_loss=0.5620 acc=0.816 (eval_time=0.82s)\n",
      "Epoch 41 | train_loss=0.4303 acc=0.851 (train_time=6.72s) | val_loss=0.5699 acc=0.815 (eval_time=0.82s)\n",
      "Epoch 42 | train_loss=0.4255 acc=0.855 (train_time=6.73s) | val_loss=0.5260 acc=0.823 (eval_time=0.78s)\n",
      " Neues bestes Modell gespeichert: best_small_cifar10_cnn.pt (val_loss=0.5260)\n",
      "Epoch 43 | train_loss=0.4140 acc=0.858 (train_time=6.64s) | val_loss=0.5379 acc=0.823 (eval_time=0.82s)\n",
      "Epoch 44 | train_loss=0.4077 acc=0.859 (train_time=6.49s) | val_loss=0.5389 acc=0.823 (eval_time=0.81s)\n",
      "Epoch 45 | train_loss=0.4030 acc=0.861 (train_time=6.56s) | val_loss=0.5596 acc=0.820 (eval_time=0.82s)\n",
      "Epoch 46 | train_loss=0.3970 acc=0.863 (train_time=6.51s) | val_loss=0.5575 acc=0.823 (eval_time=0.83s)\n",
      "Epoch 47 | train_loss=0.3907 acc=0.866 (train_time=6.67s) | val_loss=0.5845 acc=0.815 (eval_time=0.84s)\n",
      "Training beendet nach 47 Epochen (keine Verbesserung der Val-Loss fr 5 Epochen).\n",
      "Bestes Modell liegt unter: best_small_cifar10_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "# mini_cifar10_cnn_earlystop.py\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---- Setup ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CIFAR-10: 32x32 RGB, Klassen=10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "# ---- Kleines CNN ----\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)   # 32x32 -> 32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 32x32 -> 32x32\n",
    "        self.pool  = nn.MaxPool2d(2, 2)               # 32x32 -> 16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # 16x16 -> 16x16\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)# 16x16 -> 16x16\n",
    "        self.head  = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),                  # -> 128x1x1\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---- Early Stopping Setup ----\n",
    "patience = 5                 # Stoppe, wenn sich val loss 5 Epochen nicht verbessert\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "epoch = 0\n",
    "best_model_path = \"best_small_cifar10_cnn.pt\"\n",
    "\n",
    "# ---- Training Loop (ohne feste Epochen-Anzahl) ----\n",
    "while epochs_no_improve < patience:\n",
    "    epoch += 1\n",
    "\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    start_train = time.perf_counter()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_time = time.perf_counter() - start_train\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # ---- Evaluation ----\n",
    "    model.eval()\n",
    "    start_eval = time.perf_counter()\n",
    "    correct, total, test_loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss_sum += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    eval_time = time.perf_counter() - start_eval\n",
    "    val_loss = test_loss_sum / total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.3f} \"\n",
    "        f\"(train_time={train_time:.2f}s) | \"\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.3f} \"\n",
    "        f\"(eval_time={eval_time:.2f}s)\"\n",
    "    )\n",
    "\n",
    "    # ---- Early Stopping & Best Model Speichern ----\n",
    "    if val_loss < best_val_loss - 1e-6:  # kleiner Toleranzwert verhindert Flattern\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\" Neues bestes Modell gespeichert: {best_model_path} (val_loss={best_val_loss:.4f})\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "print(\n",
    "    f\"Training beendet nach {epoch} Epochen \"\n",
    "    f\"(keine Verbesserung der Val-Loss fr {patience} Epochen).\"\n",
    ")\n",
    "print(f\"Bestes Modell liegt unter: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a71d0f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b17f43fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd5787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
