{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78134ba0",
   "metadata": {},
   "source": [
    "# Using a HF model (variant 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein deutscher Assistent zur Klassifikation von Anfragen.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Wieviele Semester dauert das Studium Künstliche Intelligenz und Computer Vision?\"},\n",
    "]\n",
    "\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "out = model.generate(**tok(prompt, return_tensors=\"pt\").to(model.device), max_new_tokens=50)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee4ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8da58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok(prompt2, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68384fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5203f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f970efe",
   "metadata": {},
   "source": [
    "# Using a HF model (variant 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3c7bd",
   "metadata": {},
   "source": [
    "# Using a HF model (variant 3: with quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7657e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652efd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-11-05 11:40:58.528210: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-05 11:40:58.566264: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-05 11:40:59.461652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb7958be1b247679b3ff4413c007d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Du bist ein deutscher Assistent zur Klassifikation von Anfragen.\n",
      "\n",
      "Wieviele Semester dauert das Studium Künstliche Intelligenz und Computer Vision an der Hochschule Kempten? [/INST] In Deutschland variiert die Dauer des Studiums Künstliche Intelligenz und Computer Vision an verschiedenen Hochschulen. An der Hochschule Kempten dauert das Studium der Masterstudiengänge Computer Science mit Schwerpunkt Computer Vision und Kognitive Systeme, das auch Aspekte der Künstlichen Intelligenz behandelt, in der Regel sechs Semester. Das entspricht in der Regel zwei Jahren, aber es kann je nach Einzelheiten des Studienplans etwas länger dauern.\n",
      "\n",
      "Für das Bachelorstudium gibt es an der Hochschule Kempten keinen direkt vergleichbaren Studiengang, der die genauen Inhalte von Künstlicher Intelligenz und Computer Vision umfasst. Es gibt jedoch Studiengänge wie Informatik, die Grundkenntnisse in diesen Bereichen vermitteln und auf die ein Masterstudium folgen kann. Das Bachelorstudium dauert in der Regel acht Semester, also in der Regel vier Jahre.\n",
      "\n",
      "Es ist wichtig zu beachten, dass die genaue D\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # 4-bit Quantisierung\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # gute Wahl für LLMs\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Rechen-Dtype\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"                  # sharded auf GPU/CPU verteilen\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Du bist ein deutscher Assistent zur Klassifikation von Anfragen.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Wieviele Semester dauert das Studium Künstliche Intelligenz und Computer Vision an der Hochschule Kempten?\"},\n",
    "]\n",
    "\n",
    "inputs = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "\n",
    "# WICHTIG: Nicht model.to(\"cuda\") aufrufen. Das Modell ist bereits sharded+quantisiert.\n",
    "# Entweder Inputs auf \"cuda\" legen...\n",
    "inputs = tok.apply_chat_template(\n",
    "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    ").to(\"cuda\")  # einfach den Tensor verschieben\n",
    "\n",
    "out = model.generate(\n",
    "    input_ids=inputs,             # explizit benennen\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tok.pad_token_id\n",
    ")\n",
    "\n",
    "print(tok.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c97b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9947fc",
   "metadata": {},
   "source": [
    "# Anfragen-Klassifikation mit einem HF Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885eb6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-11-05 12:01:42.064352: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-05 12:01:42.101327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-05 12:01:43.004053: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173e79bd422348beb9e7793ada9cfe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Wieviele Semester dauert das Studium Künstliche Intelligenz und Computer Vision?\n",
      "          \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "          \n",
      "Das LLM klassifiziert diese Anfrage als: [INST] \n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Wieviele Semester dauert das Studium Künstliche Intelligenz und Computer Vision?\n",
      "          \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "           [/INST] 1.\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Ohh!! So viel zu tun! Brauche mehr Zeit bis zur Abgabe meiner Masterarbeit!\n",
      "            Können Sie die Frist bitte einen Monat nach hinten schieben!\n",
      "          \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "          \n",
      "Das LLM klassifiziert diese Anfrage als: [INST] \n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Ohh!! So viel zu tun! Brauche mehr Zeit bis zur Abgabe meiner Masterarbeit!\n",
      "            Können Sie die Frist bitte einen Monat nach hinten schieben!\n",
      "          \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "           [/INST] 2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Wir sind die Firma SuperSki3000. Wir suchen Studenten als Skilehrer.\n",
      "            Dürften wir am Eingang hierzu ein Poster bei Ihnen aufhängen?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "          \n",
      "Das LLM klassifiziert diese Anfrage als: [INST] \n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Wir sind die Firma SuperSki3000. Wir suchen Studenten als Skilehrer.\n",
      "            Dürften wir am Eingang hierzu ein Poster bei Ihnen aufhängen?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "           [/INST] 5. Anfrage Sonstiges\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Darf ich das Busticket eigentlich auch in den Semesterferien nutzen?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "          \n",
      "Das LLM klassifiziert diese Anfrage als: [INST] \n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Darf ich das Busticket eigentlich auch in den Semesterferien nutzen?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "           [/INST] 4. Anfrage wegen Semesterticket.\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Was gibt es heute in der Mensa zu essen?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "          \n",
      "Das LLM klassifiziert diese Anfrage als: [INST] \n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Was gibt es heute in der Mensa zu essen?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "           [/INST] 5. Anfrage Sonstiges.\n",
      "\n",
      "This query falls under the \"Anfrage Sonstiges\" (Miscellaneous query) category, as it is not related to a specific study program, a request for an extension on a thesis, an inquiry about job postings, or an inquiry about semester tickets.\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Darf ich mein Auto auch am Wochenende in der Tiefgarage parken?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "          \n",
      "Das LLM klassifiziert diese Anfrage als: [INST] \n",
      "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
      "            Webseite der Hochschule Kempten.\n",
      "\n",
      "            Hier die Anfrage:\n",
      "            ---\n",
      "            \n",
      "            Darf ich mein Auto auch am Wochenende in der Tiefgarage parken?\n",
      "           \n",
      "            ---\n",
      "\n",
      "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
      "            ---\n",
      "            1. Anfrage zu einem speziellem Studium\n",
      "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
      "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
      "            4. Anfrage wegen Semesterticket\n",
      "            5. Anfrage Sonstiges\n",
      "            ---\n",
      "            Antworte nur mit 1-5\n",
      "           [/INST] 5. Anfrage Sonstiges\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # 4-bit Quantisierung\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # gute Wahl für LLMs\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Rechen-Dtype\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"                  # sharded auf GPU/CPU verteilen\n",
    ")\n",
    "\n",
    "\n",
    "anfrage1 = \"\"\"\n",
    "            Wieviele Semester dauert das Studium Künstliche Intelligenz und Computer Vision?\n",
    "          \"\"\"\n",
    "\n",
    "anfrage2 = \"\"\"\n",
    "            Ohh!! So viel zu tun! Brauche mehr Zeit bis zur Abgabe meiner Masterarbeit!\n",
    "            Können Sie die Frist bitte einen Monat nach hinten schieben!\n",
    "          \"\"\"\n",
    "\n",
    "anfrage3 = \"\"\"\n",
    "            Wir sind die Firma SuperSki3000. Wir suchen Studenten als Skilehrer.\n",
    "            Dürften wir am Eingang hierzu ein Poster bei Ihnen aufhängen?\n",
    "           \"\"\"\n",
    "\n",
    "anfrage4 = \"\"\"\n",
    "            Darf ich das Busticket eigentlich auch in den Semesterferien nutzen?\n",
    "           \"\"\"\n",
    "\n",
    "anfrage5 = \"\"\"\n",
    "            Was gibt es heute in der Mensa zu essen?\n",
    "           \"\"\"\n",
    "\n",
    "anfrage6 = \"\"\"\n",
    "            Darf ich mein Auto auch am Wochenende in der Tiefgarage parken?\n",
    "           \"\"\"\n",
    "\n",
    "anfragen = [anfrage1,\n",
    "            anfrage2,\n",
    "            anfrage3,\n",
    "            anfrage4,\n",
    "            anfrage5,\n",
    "            anfrage6 ]\n",
    "\n",
    "for anfrage in anfragen:\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "            Du bist ein Assistent zur Klassifikation von Benutzeranfragen auf der\n",
    "            Webseite der Hochschule Kempten.\n",
    "\n",
    "            Hier die Anfrage:\n",
    "            ---\n",
    "            {anfrage}\n",
    "            ---\n",
    "\n",
    "            Du sollst jede Anfrage klassifizieren in eine der folgenden 5 Klassen:\n",
    "            ---\n",
    "            1. Anfrage zu einem speziellem Studium\n",
    "            2. Antrag auf Fristverlängerung für Abschlussarbeit\n",
    "            3. Anfrage wegen Veröffentlichung Stellenangebote\n",
    "            4. Anfrage wegen Semesterticket\n",
    "            5. Anfrage Sonstiges\n",
    "            ---\n",
    "            Antworte nur mit 1-5\n",
    "          \"\"\"\n",
    "  \n",
    "  print(\"*\" * 100)\n",
    "  print(prompt)\n",
    "\n",
    "\n",
    "  messages = [      \n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "  ]\n",
    "\n",
    "  inputs = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "\n",
    "  # WICHTIG: Nicht model.to(\"cuda\") aufrufen. Das Modell ist bereits sharded+quantisiert.\n",
    "  # Entweder Inputs auf \"cuda\" legen...\n",
    "  inputs = tok.apply_chat_template(\n",
    "      messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    "  ).to(\"cuda\")  # einfach den Tensor verschieben\n",
    "\n",
    "  \n",
    "  out = model.generate(\n",
    "      input_ids=inputs,             # explizit benennen\n",
    "      max_new_tokens=256,\n",
    "      do_sample=True,\n",
    "      temperature=0.7,\n",
    "      top_p=0.9,\n",
    "      pad_token_id=tok.pad_token_id\n",
    "  )\n",
    "\n",
    "  resp = tok.decode(out[0], skip_special_tokens=True)\n",
    "  print(f\"Das LLM klassifiziert diese Anfrage als: {resp}\")\n",
    "  print(\"*\" * 100)\n",
    "  print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e45be",
   "metadata": {},
   "source": [
    "# Textsegmentierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd6e15f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b336fea65a40f7bd6f04155e484eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\n",
      "            Du bist ein Assistent der in Freitextfeldern von technischen Außendienstmitarbeitern\n",
      "            die Sätze identifizieren soll, die beschreiben,\n",
      "            1. was für ein Problem beim Kunden vorlag\n",
      "            2. was der Außendienstmitarbeiter für eine Lösung durchgeführt hat\n",
      "\n",
      "            Hier der Text des Außendienstmitarbeiters\n",
      "            ---\n",
      "            \n",
      "            Ich bin zum Kunden nach Oberstaufen gefahren. Im Transformatorraum\n",
      "            roch es verschmort und einer der Adapter war auch zu heiß.\n",
      "            Der Adapter X17 war kaputt. Habe ihn deswegen durch\n",
      "            einen neuen Adapter vom Typ X19-200 ausgetauscht und habe die\n",
      "            Steuerung der Seilbahn noch hoch gefahren. Jetzt läuft die Seilbahn\n",
      "            wieder perfekt! Habe in der Kantine vom Kunden gegessen. Dort gab es leckere\n",
      "            Spaghetti Bolognese! Viele Grüße, Hans!\n",
      "          \n",
      "            ---\n",
      "\n",
      "            Du sollst jeden Text segmentieren und folgenden Output erzeugen:\n",
      "            ---\n",
      "            Problembeschreibung: <alle Sätze, die das Problem beschreiben>\n",
      "            Lösungsbeschreibung: <alle Sätze, die die Lösung beschreiben>\n",
      "            Irrelevantes:  <alle Sätze, die weder das Problem oder die Lösung beschreiben>\n",
      "            ---            \n",
      "          \n",
      "LLM AUSGABE: [INST] \n",
      "            Du bist ein Assistent der in Freitextfeldern von technischen Außendienstmitarbeitern\n",
      "            die Sätze identifizieren soll, die beschreiben,\n",
      "            1. was für ein Problem beim Kunden vorlag\n",
      "            2. was der Außendienstmitarbeiter für eine Lösung durchgeführt hat\n",
      "\n",
      "            Hier der Text des Außendienstmitarbeiters\n",
      "            ---\n",
      "            \n",
      "            Ich bin zum Kunden nach Oberstaufen gefahren. Im Transformatorraum\n",
      "            roch es verschmort und einer der Adapter war auch zu heiß.\n",
      "            Der Adapter X17 war kaputt. Habe ihn deswegen durch\n",
      "            einen neuen Adapter vom Typ X19-200 ausgetauscht und habe die\n",
      "            Steuerung der Seilbahn noch hoch gefahren. Jetzt läuft die Seilbahn\n",
      "            wieder perfekt! Habe in der Kantine vom Kunden gegessen. Dort gab es leckere\n",
      "            Spaghetti Bolognese! Viele Grüße, Hans!\n",
      "          \n",
      "            ---\n",
      "\n",
      "            Du sollst jeden Text segmentieren und folgenden Output erzeugen:\n",
      "            ---\n",
      "            Problembeschreibung: <alle Sätze, die das Problem beschreiben>\n",
      "            Lösungsbeschreibung: <alle Sätze, die die Lösung beschreiben>\n",
      "            Irrelevantes:  <alle Sätze, die weder das Problem oder die Lösung beschreiben>\n",
      "            ---            \n",
      "           [/INST] Problembeschreibung: Im Transformatorraum roch es verschmort und einer der Adapters war auch zu heiss. Der Adapter X17 war kaputt.\n",
      "Lösungsbeschreibung: Habe durch den neuen Adapter vom Typ X19-200 ausgetauscht und habe die Steuerung der Seilbahn noch hoch gefahren. Jetzt läuft die Seilbahn wieder perfekt!\n",
      "Irrelevantes: Ich bin zum Kunden nach Oberstaufen gefahren. Habe in der Kantine vom Kunden gegessen. Dort gab es leckere Spaghetti Bolognese! Viel Grüße, Hans!\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # 4-bit Quantisierung\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # gute Wahl für LLMs\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Rechen-Dtype\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"                  # sharded auf GPU/CPU verteilen\n",
    ")\n",
    "\n",
    "\n",
    "txt1 = \"\"\"\n",
    "            Ich bin zum Kunden nach Oberstaufen gefahren. Im Transformatorraum\n",
    "            roch es verschmort und einer der Adapter war auch zu heiß.\n",
    "            Der Adapter X17 war kaputt. Habe ihn deswegen durch\n",
    "            einen neuen Adapter vom Typ X19-200 ausgetauscht und habe die\n",
    "            Steuerung der Seilbahn noch hoch gefahren. Jetzt läuft die Seilbahn\n",
    "            wieder perfekt! Habe in der Kantine vom Kunden gegessen. Dort gab es leckere\n",
    "            Spaghetti Bolognese! Viele Grüße, Hans!\n",
    "          \"\"\"\n",
    "\n",
    "texte = [txt1]\n",
    "\n",
    "for txt in texte:\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "            Du bist ein Assistent der in Freitextfeldern von technischen Außendienstmitarbeitern\n",
    "            die Sätze identifizieren soll, die beschreiben,\n",
    "            1. was für ein Problem beim Kunden vorlag\n",
    "            2. was der Außendienstmitarbeiter für eine Lösung durchgeführt hat\n",
    "            \n",
    "            Hier der Text des Außendienstmitarbeiters\n",
    "            ---\n",
    "            {txt}\n",
    "            ---\n",
    "\n",
    "            Du sollst jeden Text segmentieren und folgenden Output erzeugen:\n",
    "            ---\n",
    "            Problembeschreibung: <alle Sätze, die das Problem beschreiben>\n",
    "            Lösungsbeschreibung: <alle Sätze, die die Lösung beschreiben>\n",
    "            Irrelevantes:  <alle Sätze, die weder das Problem oder die Lösung beschreiben>\n",
    "            ---            \n",
    "          \"\"\"\n",
    "  \n",
    "  print(\"*\" * 100)\n",
    "  print(prompt)\n",
    "\n",
    "\n",
    "  messages = [      \n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "  ]\n",
    "\n",
    "  inputs = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "\n",
    "  # WICHTIG: Nicht model.to(\"cuda\") aufrufen. Das Modell ist bereits sharded+quantisiert.\n",
    "  # Entweder Inputs auf \"cuda\" legen...\n",
    "  inputs = tok.apply_chat_template(\n",
    "      messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    "  ).to(\"cuda\")  # einfach den Tensor verschieben\n",
    "\n",
    "  \n",
    "  out = model.generate(\n",
    "      input_ids=inputs,             # explizit benennen\n",
    "      max_new_tokens=256,\n",
    "      do_sample=True,\n",
    "      temperature=0.7,\n",
    "      top_p=0.9,\n",
    "      pad_token_id=tok.pad_token_id\n",
    "  )\n",
    "\n",
    "  resp = tok.decode(out[0], skip_special_tokens=True)\n",
    "  print(f\"LLM AUSGABE: {resp}\")\n",
    "  print(\"*\" * 100)\n",
    "  print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd43ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
