{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a9a233",
   "metadata": {},
   "source": [
    "# The Vanishing Gradients Problem"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72df41f2",
   "metadata": {},
   "source": [
    "The vanishing gradients problem means that gradients become extremely small.\n",
    "\n",
    "They can become so small that weight updates are almost zero.\n",
    "\n",
    "As a result, the network stops learning, especially in the early (lower) layers.\n",
    "\n",
    "Over the years, several techniques were developed to deal with vanishing gradients:\n",
    "\n",
    "- ReLU activations (and variants like Leaky ReLU, ELU, GELU):\n",
    "  They don’t squash values into a small range like sigmoid/tanh, so gradients don’t shrink as much.\n",
    "\n",
    "- Better weight initialization:\n",
    "  Methods like He initialization or Xavier/Glorot initialization ensure that activations and gradients have roughly the same variance across layers\n",
    "\n",
    "- Batch normalization:\n",
    "  Normalizes activations within each batch, keeping gradients in a reasonable range.\n",
    "\n",
    "- Residual connections (ResNets):\n",
    "  Add shortcut connections that let gradients flow directly through layers, drastically reducing the vanishing effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5217f",
   "metadata": {},
   "source": [
    "# Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36dab0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: mean |grad| = 0.0000000000\n",
      "Layer 2: mean |grad| = 0.0000000000\n",
      "Layer 3: mean |grad| = 0.0000000000\n",
      "Layer 4: mean |grad| = 0.0000000000\n",
      "Layer 5: mean |grad| = 0.0000000000\n",
      "Layer 6: mean |grad| = 0.0000000000\n",
      "Layer 7: mean |grad| = 0.0000000000\n",
      "Layer 8: mean |grad| = 0.0000000000\n",
      "Layer 9: mean |grad| = 0.0000000000\n",
      "Layer 10: mean |grad| = 0.0000000000\n",
      "Layer 11: mean |grad| = 0.0000000002\n",
      "Layer 12: mean |grad| = 0.0000000012\n",
      "Layer 13: mean |grad| = 0.0000000074\n",
      "Layer 14: mean |grad| = 0.0000000617\n",
      "Layer 15: mean |grad| = 0.0000004269\n",
      "Layer 16: mean |grad| = 0.0000029049\n",
      "Layer 17: mean |grad| = 0.0000205589\n",
      "Layer 18: mean |grad| = 0.0001422858\n",
      "Layer 19: mean |grad| = 0.0011019657\n",
      "Layer 20: mean |grad| = 0.0079036895\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a deep network with sigmoid activations\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, depth=20):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(100, 100))\n",
    "            layers.append(nn.Sigmoid())  # <- prone to vanishing gradients\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output(self.net(x))\n",
    "\n",
    "# Create random input\n",
    "x = torch.randn(1, 100)\n",
    "model = DeepNet(depth=20)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Forward + backward pass\n",
    "target = torch.tensor([[1.0]])\n",
    "output = model(x)\n",
    "loss = loss_fn(output, target)\n",
    "loss.backward()\n",
    "\n",
    "# Inspect average gradient magnitude per layer\n",
    "for i, layer in enumerate(model.net):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        #print(layer.weight.grad)\n",
    "        grad_mean = layer.weight.grad.abs().mean().item()\n",
    "        print(f\"Layer {i//2 + 1}: mean |grad| = {grad_mean:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f158fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): Sigmoid()\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): Sigmoid()\n",
       "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (9): Sigmoid()\n",
       "    (10): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (11): Sigmoid()\n",
       "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (13): Sigmoid()\n",
       "    (14): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (15): Sigmoid()\n",
       "    (16): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (17): Sigmoid()\n",
       "    (18): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (19): Sigmoid()\n",
       "    (20): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (21): Sigmoid()\n",
       "    (22): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (23): Sigmoid()\n",
       "    (24): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (25): Sigmoid()\n",
       "    (26): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (27): Sigmoid()\n",
       "    (28): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (29): Sigmoid()\n",
       "    (30): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (31): Sigmoid()\n",
       "    (32): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (33): Sigmoid()\n",
       "    (34): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (35): Sigmoid()\n",
       "    (36): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (37): Sigmoid()\n",
       "    (38): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (39): Sigmoid()\n",
       "  )\n",
       "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52d819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
