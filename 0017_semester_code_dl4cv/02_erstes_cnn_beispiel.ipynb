{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062bd1c4",
   "metadata": {},
   "source": [
    "# Mini-Beispiel für CNN mit PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857acfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "num_samples=5589\n",
      "Klassen (7): ['Auto Rickshaws', 'Bikes', 'Cars', 'Motorcycles', 'Planes', 'Ships', 'Trains']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | Train Loss: 1.3530 Acc: 0.502 | Val Loss: 1.1047 Acc: 0.594\n",
      "✓ Bestes Modell aktualisiert: best_cnn.pth (Val Acc 0.594)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/10 | Train Loss: 1.0688 Acc: 0.624 | Val Loss: 1.6293 Acc: 0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/10 | Train Loss: 0.9246 Acc: 0.680 | Val Loss: 0.8254 Acc: 0.705\n",
      "✓ Bestes Modell aktualisiert: best_cnn.pth (Val Acc 0.705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/10 | Train Loss: 0.8567 Acc: 0.702 | Val Loss: 0.9264 Acc: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/10 | Train Loss: 0.7748 Acc: 0.737 | Val Loss: 1.0822 Acc: 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/10 | Train Loss: 0.7335 Acc: 0.748 | Val Loss: 0.9480 Acc: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07/10 | Train Loss: 0.6821 Acc: 0.763 | Val Loss: 0.7631 Acc: 0.724\n",
      "✓ Bestes Modell aktualisiert: best_cnn.pth (Val Acc 0.724)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/10 | Train Loss: 0.6362 Acc: 0.782 | Val Loss: 0.9450 Acc: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09/10 | Train Loss: 0.6007 Acc: 0.785 | Val Loss: 0.5945 Acc: 0.782\n",
      "✓ Bestes Modell aktualisiert: best_cnn.pth (Val Acc 0.782)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.5843 Acc: 0.798 | Val Loss: 0.5220 Acc: 0.833\n",
      "✓ Bestes Modell aktualisiert: best_cnn.pth (Val Acc 0.833)\n",
      "Training fertig.\n",
      "Bestes Val-Accuracy: 0.833\n"
     ]
    }
   ],
   "source": [
    "# train_cnn.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_loaders(root, img_size=224, batch_size=32, val_split=0.2, seed=42, num_workers=1):\n",
    "    root = Path(root)\n",
    "\n",
    "    # Transforms: leichte Augmentierung für Training, nur Resize+Norm fürs Validieren\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    val_tfms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    # Zwei ImageFolder-Instanzen auf das gleiche Root, aber mit unterschiedlichen Transforms\n",
    "    train_dataset = datasets.ImageFolder(root=str(root), transform=train_tfms)\n",
    "    val_dataset   = datasets.ImageFolder(root=str(root), transform=val_tfms)\n",
    "\n",
    "    # Reproduzierbarer Index-Split\n",
    "    num_samples = len(train_dataset)\n",
    "    print(f\"{num_samples=}\")\n",
    "    indices = np.arange(num_samples)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    split = int(np.floor(val_split * num_samples))\n",
    "    val_idx = indices[:split]\n",
    "    train_idx = indices[split:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler   = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader, train_dataset.classes\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        # Kleines, effizientes CNN\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def train(root=\"data\", epochs=10, img_size=224, batch_size=32, lr=1e-3, val_split=0.2, seed=42):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    train_loader, val_loader, classes = get_loaders(\n",
    "        root=root, img_size=img_size, batch_size=batch_size, val_split=val_split, seed=seed\n",
    "    )\n",
    "    num_classes = len(classes)\n",
    "    print(f\"Klassen ({num_classes}): {classes}\")\n",
    "\n",
    "    model = SimpleCNN(num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_path = \"best_cnn.pth\"\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            running_total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / running_total\n",
    "        train_acc = running_correct / running_total\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.3f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\"model_state\": model.state_dict(),\n",
    "                        \"classes\": classes,\n",
    "                        \"img_size\": img_size}, best_path)\n",
    "            print(f\"✓ Bestes Modell aktualisiert: {best_path} (Val Acc {best_val_acc:.3f})\")\n",
    "\n",
    "    print(\"Training fertig.\")\n",
    "    print(f\"Bestes Val-Accuracy: {best_val_acc:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Passe 'root' an deinen Datenordner an:\n",
    "    # Struktur: root/\n",
    "    #   ├── klasse_a/\n",
    "    #   │     ├── img1.jpg\n",
    "    #   │     └── ...\n",
    "    #   └── klasse_b/\n",
    "    #         └── ...\n",
    "    dataset_root = \"/home/juebrauer/link_to_vcd/10_datasets/57_vehicle_image_classification/Vehicles\"\n",
    "    train(root=dataset_root,\n",
    "          epochs=10,\n",
    "          img_size=224,\n",
    "          batch_size=32,\n",
    "          lr=1e-3,\n",
    "          val_split=0.2,\n",
    "          seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fdbf7",
   "metadata": {},
   "source": [
    "# Qualitative Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa1548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--root ROOT] [--ckpt CKPT] [--seed SEED]\n",
      "                             [--num NUM]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1000/jupyter/runtime/kernel-v37bb6a0e21f1955c1ab94e176c95d4443d7714bb7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juebrauer/miniconda3/envs/env_teaching/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# viz_25_samples.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- SimpleCNN (wie im Trainingsskript) -------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--root\", type=str, default=\"data\", help=\"Wurzelordner mit Unterordnern pro Klasse\")\n",
    "    parser.add_argument(\"--ckpt\", type=str, default=\"best_cnn.pth\", help=\"Pfad zum gespeicherten Checkpoint\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Zufallssamen\")\n",
    "    parser.add_argument(\"--num\", type=int, default=25, help=\"Anzahl zufälliger Bilder (zeigt 5x5 Grid)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Checkpoint laden (enthält img_size & Klassen)\n",
    "    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n",
    "    classes = ckpt[\"classes\"]\n",
    "    img_size = ckpt[\"img_size\"]\n",
    "\n",
    "    # Transforms: fürs Inferenz-Input (wie Validation)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "    infer_tfms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    # Fürs Anzeigen (ohne Norm, gleiche Größe)\n",
    "    display_tfms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "    ])\n",
    "\n",
    "    # Dataset (liest Dateipfade & Targets)\n",
    "    ds = datasets.ImageFolder(root=args.root)\n",
    "\n",
    "    # Modell wiederherstellen\n",
    "    model = SimpleCNN(num_classes=len(classes))\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # 25 (oder weniger) zufällige Indizes ziehen\n",
    "    n = min(args.num, len(ds))\n",
    "    indices = random.sample(range(len(ds)), n)\n",
    "\n",
    "    # Vorbereiten der Figur 5x5\n",
    "    rows, cols = 5, 5\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Durch zufällige Bilder iterieren\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            path, true_label = ds.samples[idx]  # (Pfad, Klassenindex)\n",
    "            # Bild fürs Modell\n",
    "            img_in = infer_tfms(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            logits = model(img_in)\n",
    "            pred_idx = logits.argmax(1).item()\n",
    "            pred_name = classes[pred_idx]\n",
    "\n",
    "            # Bild fürs Anzeigen\n",
    "            disp_img = display_tfms(Image.open(path).convert(\"RGB\"))\n",
    "            ax = axes[i]\n",
    "            ax.imshow(disp_img)\n",
    "            # Titel mit Vorhersage (und wahrem Label)\n",
    "            title = f\"Pred: {pred_name}\\nTrue: {classes[true_label]}\"\n",
    "            ax.set_title(title, fontsize=9)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # Leere Achsen ausblenden, falls < 25 Bilder\n",
    "        for j in range(i+1, rows*cols):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"25 zufällige Vorhersagen (5x5)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    out_path = Path(\"predictions_5x5.png\")\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    print(f\"Gespeichert unter: {out_path.resolve()}\")\n",
    "    plt.show()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fbdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
